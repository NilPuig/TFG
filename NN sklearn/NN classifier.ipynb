{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "galaxy=pd.read_csv(\"cat_gal_04.csv\")\n",
    "basename='flux_nb'\n",
    "initialnumber=455;\n",
    "requiredColumns=[]\n",
    "for i in range(40):\n",
    "    requiredColumns.append(basename+str(initialnumber+10*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "star=pd.read_csv(\"cat_star_04.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starTraining=pd.DataFrame()\n",
    "galaxyTraining=pd.DataFrame()\n",
    "for i in requiredColumns:\n",
    "    starTraining[i]=star[i]\n",
    "    galaxyTraining[i]=galaxy[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starTraining['class']=0\n",
    "galaxyTraining['class']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames=[starTraining,galaxyTraining]\n",
    "combined=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined=combined.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=combined.tail(3000)\n",
    "combined=combined.head(34940)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target=combined['class']\n",
    "test_target=test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del combined['class']\n",
    "del test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test=test.values\n",
    "y_test=test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=combined.values\n",
    "y=target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=40, units=100)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 100, init = 'uniform', activation = 'relu', input_dim = 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=30, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 30, init = 'uniform', activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27952 samples, validate on 6988 samples\n",
      "Epoch 1/1000\n",
      "27952/27952 [==============================] - 4s - loss: 0.4329 - acc: 0.8335 - val_loss: 0.4124 - val_acc: 0.8349\n",
      "Epoch 2/1000\n",
      "27952/27952 [==============================] - 4s - loss: 0.3916 - acc: 0.8486 - val_loss: 0.3611 - val_acc: 0.8599\n",
      "Epoch 3/1000\n",
      "27952/27952 [==============================] - 4s - loss: 0.3547 - acc: 0.8680 - val_loss: 0.3384 - val_acc: 0.8701\n",
      "Epoch 4/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.3393 - acc: 0.8726 - val_loss: 0.3267 - val_acc: 0.8734\n",
      "Epoch 5/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.3321 - acc: 0.8762 - val_loss: 0.3268 - val_acc: 0.8725\n",
      "Epoch 6/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.3278 - acc: 0.8770 - val_loss: 0.3166 - val_acc: 0.8766\n",
      "Epoch 7/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.3217 - acc: 0.8796 - val_loss: 0.3042 - val_acc: 0.8818\n",
      "Epoch 8/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.3164 - acc: 0.8820 - val_loss: 0.3775 - val_acc: 0.8729\n",
      "Epoch 9/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.3100 - acc: 0.8843 - val_loss: 0.2913 - val_acc: 0.8865\n",
      "Epoch 10/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.3089 - acc: 0.8850 - val_loss: 0.3538 - val_acc: 0.8797\n",
      "Epoch 11/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.3033 - acc: 0.8874 - val_loss: 0.2807 - val_acc: 0.8960\n",
      "Epoch 12/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.3038 - acc: 0.8879 - val_loss: 0.3116 - val_acc: 0.8944\n",
      "Epoch 13/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.2970 - acc: 0.8902 - val_loss: 0.2784 - val_acc: 0.9023\n",
      "Epoch 14/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.2936 - acc: 0.8912 - val_loss: 0.2920 - val_acc: 0.8950\n",
      "Epoch 15/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.2950 - acc: 0.8910 - val_loss: 0.3216 - val_acc: 0.8839\n",
      "Epoch 16/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.2889 - acc: 0.8921 - val_loss: 0.2782 - val_acc: 0.8975\n",
      "Epoch 17/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.2873 - acc: 0.8949 - val_loss: 0.2758 - val_acc: 0.8951\n",
      "Epoch 18/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.2857 - acc: 0.8946 - val_loss: 0.2672 - val_acc: 0.9001\n",
      "Epoch 19/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.2869 - acc: 0.8940 - val_loss: 0.2771 - val_acc: 0.9047\n",
      "Epoch 20/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.2803 - acc: 0.8959 - val_loss: 0.2761 - val_acc: 0.8984\n",
      "Epoch 21/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.2800 - acc: 0.8967 - val_loss: 0.2613 - val_acc: 0.9043\n",
      "Epoch 22/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2791 - acc: 0.8959 - val_loss: 0.2856 - val_acc: 0.9008\n",
      "Epoch 23/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2797 - acc: 0.8969 - val_loss: 0.2839 - val_acc: 0.8970\n",
      "Epoch 24/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2756 - acc: 0.8981 - val_loss: 0.2653 - val_acc: 0.8955\n",
      "Epoch 25/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2704 - acc: 0.8987 - val_loss: 0.2771 - val_acc: 0.8984\n",
      "Epoch 26/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2706 - acc: 0.8989 - val_loss: 0.3154 - val_acc: 0.8928\n",
      "Epoch 27/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2708 - acc: 0.8983 - val_loss: 0.2705 - val_acc: 0.8998\n",
      "Epoch 28/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2723 - acc: 0.8991 - val_loss: 0.2873 - val_acc: 0.8915\n",
      "Epoch 29/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2738 - acc: 0.8980 - val_loss: 0.2555 - val_acc: 0.9087\n",
      "Epoch 30/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2710 - acc: 0.8991 - val_loss: 0.2526 - val_acc: 0.9073\n",
      "Epoch 31/1000\n",
      "27952/27952 [==============================] - 11s - loss: 0.2691 - acc: 0.9000 - val_loss: 0.2534 - val_acc: 0.9103\n",
      "Epoch 32/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2636 - acc: 0.9014 - val_loss: 0.2541 - val_acc: 0.9137\n",
      "Epoch 33/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2659 - acc: 0.9012 - val_loss: 0.2825 - val_acc: 0.8990\n",
      "Epoch 34/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2638 - acc: 0.9014 - val_loss: 0.2984 - val_acc: 0.8951\n",
      "Epoch 35/1000\n",
      "27952/27952 [==============================] - 11s - loss: 0.2626 - acc: 0.9034 - val_loss: 0.2487 - val_acc: 0.9070\n",
      "Epoch 36/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2613 - acc: 0.9017 - val_loss: 0.2601 - val_acc: 0.9033\n",
      "Epoch 37/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2627 - acc: 0.9036 - val_loss: 0.2520 - val_acc: 0.9058\n",
      "Epoch 38/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2608 - acc: 0.9029 - val_loss: 0.2727 - val_acc: 0.8957\n",
      "Epoch 39/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2628 - acc: 0.9026 - val_loss: 0.2497 - val_acc: 0.9054\n",
      "Epoch 40/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2605 - acc: 0.9030 - val_loss: 0.2845 - val_acc: 0.8963\n",
      "Epoch 41/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2616 - acc: 0.9034 - val_loss: 0.2469 - val_acc: 0.9078\n",
      "Epoch 42/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2643 - acc: 0.9015 - val_loss: 0.2633 - val_acc: 0.9058\n",
      "Epoch 43/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2616 - acc: 0.9026 - val_loss: 0.2454 - val_acc: 0.9121\n",
      "Epoch 44/1000\n",
      "27952/27952 [==============================] - 11s - loss: 0.2566 - acc: 0.9061 - val_loss: 0.2708 - val_acc: 0.9030\n",
      "Epoch 45/1000\n",
      "27952/27952 [==============================] - 11s - loss: 0.2560 - acc: 0.9042 - val_loss: 0.2407 - val_acc: 0.9159\n",
      "Epoch 46/1000\n",
      "27952/27952 [==============================] - 11s - loss: 0.2558 - acc: 0.9060 - val_loss: 0.2382 - val_acc: 0.9118\n",
      "Epoch 47/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2571 - acc: 0.9055 - val_loss: 0.2563 - val_acc: 0.9038\n",
      "Epoch 48/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2535 - acc: 0.9063 - val_loss: 0.2388 - val_acc: 0.9130\n",
      "Epoch 49/1000\n",
      "27952/27952 [==============================] - 11s - loss: 0.2547 - acc: 0.9056 - val_loss: 0.2411 - val_acc: 0.9160\n",
      "Epoch 50/1000\n",
      "27952/27952 [==============================] - 11s - loss: 0.2511 - acc: 0.9071 - val_loss: 0.2683 - val_acc: 0.9005\n",
      "Epoch 51/1000\n",
      "27952/27952 [==============================] - 12s - loss: 0.2496 - acc: 0.9068 - val_loss: 0.2471 - val_acc: 0.9124\n",
      "Epoch 52/1000\n",
      "27952/27952 [==============================] - 11s - loss: 0.2533 - acc: 0.9052 - val_loss: 0.3158 - val_acc: 0.8775\n",
      "Epoch 53/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2504 - acc: 0.9061 - val_loss: 0.2351 - val_acc: 0.9137\n",
      "Epoch 54/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2485 - acc: 0.9068 - val_loss: 0.2334 - val_acc: 0.9123\n",
      "Epoch 55/1000\n",
      "27952/27952 [==============================] - 11s - loss: 0.2445 - acc: 0.9092 - val_loss: 0.2702 - val_acc: 0.8997\n",
      "Epoch 56/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2478 - acc: 0.9075 - val_loss: 0.2470 - val_acc: 0.9084\n",
      "Epoch 57/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2429 - acc: 0.9087 - val_loss: 0.2339 - val_acc: 0.9194\n",
      "Epoch 58/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2475 - acc: 0.9090 - val_loss: 0.2264 - val_acc: 0.9179\n",
      "Epoch 59/1000\n",
      "27952/27952 [==============================] - 11s - loss: 0.2421 - acc: 0.9104 - val_loss: 0.2359 - val_acc: 0.9076\n",
      "Epoch 60/1000\n",
      "27952/27952 [==============================] - 12s - loss: 0.2445 - acc: 0.9085 - val_loss: 0.2354 - val_acc: 0.9129\n",
      "Epoch 61/1000\n",
      "27952/27952 [==============================] - 12s - loss: 0.2410 - acc: 0.9105 - val_loss: 0.2396 - val_acc: 0.9086\n",
      "Epoch 62/1000\n",
      "27952/27952 [==============================] - 11s - loss: 0.2409 - acc: 0.9086 - val_loss: 0.2461 - val_acc: 0.9103\n",
      "Epoch 63/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2410 - acc: 0.9097 - val_loss: 0.2297 - val_acc: 0.9189\n",
      "Epoch 64/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2405 - acc: 0.9095 - val_loss: 0.2290 - val_acc: 0.9090\n",
      "Epoch 65/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2393 - acc: 0.9112 - val_loss: 0.2246 - val_acc: 0.9189\n",
      "Epoch 66/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2394 - acc: 0.9105 - val_loss: 0.2654 - val_acc: 0.9005\n",
      "Epoch 67/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2374 - acc: 0.9111 - val_loss: 0.2306 - val_acc: 0.9164\n",
      "Epoch 68/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.2387 - acc: 0.9096 - val_loss: 0.2298 - val_acc: 0.9157\n",
      "Epoch 69/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2334 - acc: 0.9122 - val_loss: 0.2302 - val_acc: 0.9193\n",
      "Epoch 70/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.2339 - acc: 0.9118 - val_loss: 0.2176 - val_acc: 0.9184\n",
      "Epoch 71/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2354 - acc: 0.9133 - val_loss: 0.2225 - val_acc: 0.9170\n",
      "Epoch 72/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2353 - acc: 0.9119 - val_loss: 0.2298 - val_acc: 0.9129\n",
      "Epoch 73/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2387 - acc: 0.9102 - val_loss: 0.2230 - val_acc: 0.9173\n",
      "Epoch 74/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2308 - acc: 0.9117 - val_loss: 0.2523 - val_acc: 0.9094\n",
      "Epoch 75/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2305 - acc: 0.9127 - val_loss: 0.2339 - val_acc: 0.9098\n",
      "Epoch 76/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2336 - acc: 0.9123 - val_loss: 0.2132 - val_acc: 0.9233\n",
      "Epoch 77/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2301 - acc: 0.9123 - val_loss: 0.2186 - val_acc: 0.9219\n",
      "Epoch 78/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2328 - acc: 0.9125 - val_loss: 0.2263 - val_acc: 0.9203\n",
      "Epoch 79/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2292 - acc: 0.9127 - val_loss: 0.2145 - val_acc: 0.9206\n",
      "Epoch 80/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.2345 - acc: 0.9127 - val_loss: 0.2364 - val_acc: 0.9123\n",
      "Epoch 81/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2311 - acc: 0.9120 - val_loss: 0.2198 - val_acc: 0.9170\n",
      "Epoch 82/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.2280 - acc: 0.9136 - val_loss: 0.2464 - val_acc: 0.9050\n",
      "Epoch 83/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2265 - acc: 0.9140 - val_loss: 0.2328 - val_acc: 0.9106\n",
      "Epoch 84/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.2281 - acc: 0.9130 - val_loss: 0.2133 - val_acc: 0.9223\n",
      "Epoch 85/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.2276 - acc: 0.9127 - val_loss: 0.2860 - val_acc: 0.8990\n",
      "Epoch 86/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2248 - acc: 0.9156 - val_loss: 0.2189 - val_acc: 0.9244\n",
      "Epoch 87/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2268 - acc: 0.9127 - val_loss: 0.2234 - val_acc: 0.9149\n",
      "Epoch 88/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2276 - acc: 0.9144 - val_loss: 0.2307 - val_acc: 0.9176\n",
      "Epoch 89/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.2279 - acc: 0.9142 - val_loss: 0.2354 - val_acc: 0.9114\n",
      "Epoch 90/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2294 - acc: 0.9120 - val_loss: 0.3653 - val_acc: 0.8855\n",
      "Epoch 91/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2266 - acc: 0.9139 - val_loss: 0.2165 - val_acc: 0.9199\n",
      "Epoch 92/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.2281 - acc: 0.9138 - val_loss: 0.2145 - val_acc: 0.9210\n",
      "Epoch 93/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2224 - acc: 0.9144 - val_loss: 0.2524 - val_acc: 0.9187\n",
      "Epoch 94/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.2248 - acc: 0.9144 - val_loss: 0.2601 - val_acc: 0.8958\n",
      "Epoch 95/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2218 - acc: 0.9154 - val_loss: 0.2077 - val_acc: 0.9213\n",
      "Epoch 96/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2192 - acc: 0.9166 - val_loss: 0.2028 - val_acc: 0.9216\n",
      "Epoch 97/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2205 - acc: 0.9170 - val_loss: 0.2746 - val_acc: 0.9030\n",
      "Epoch 98/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2226 - acc: 0.9146 - val_loss: 0.2730 - val_acc: 0.8991\n",
      "Epoch 99/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2226 - acc: 0.9143 - val_loss: 0.2140 - val_acc: 0.9184\n",
      "Epoch 100/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2192 - acc: 0.9157 - val_loss: 0.2135 - val_acc: 0.9197\n",
      "Epoch 101/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2195 - acc: 0.9163 - val_loss: 0.2060 - val_acc: 0.9222\n",
      "Epoch 102/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2209 - acc: 0.9157 - val_loss: 0.2192 - val_acc: 0.9163\n",
      "Epoch 103/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2207 - acc: 0.9171 - val_loss: 0.2516 - val_acc: 0.9057\n",
      "Epoch 104/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2192 - acc: 0.9156 - val_loss: 0.2071 - val_acc: 0.9247\n",
      "Epoch 105/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2199 - acc: 0.9162 - val_loss: 0.2571 - val_acc: 0.8954\n",
      "Epoch 106/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2153 - acc: 0.9167 - val_loss: 0.2034 - val_acc: 0.9266\n",
      "Epoch 107/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.2177 - acc: 0.9177 - val_loss: 0.2039 - val_acc: 0.9286\n",
      "Epoch 108/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2192 - acc: 0.9163 - val_loss: 0.2139 - val_acc: 0.9267\n",
      "Epoch 109/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2144 - acc: 0.9166 - val_loss: 0.2033 - val_acc: 0.9223\n",
      "Epoch 110/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.2183 - acc: 0.9155 - val_loss: 0.2122 - val_acc: 0.9203\n",
      "Epoch 111/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.2155 - acc: 0.9157 - val_loss: 0.2150 - val_acc: 0.9193\n",
      "Epoch 112/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.2174 - acc: 0.9155 - val_loss: 0.2201 - val_acc: 0.9227\n",
      "Epoch 113/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.2147 - acc: 0.9172 - val_loss: 0.2150 - val_acc: 0.9179\n",
      "Epoch 114/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.2165 - acc: 0.9164 - val_loss: 0.1983 - val_acc: 0.9274\n",
      "Epoch 115/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2130 - acc: 0.9177 - val_loss: 0.2160 - val_acc: 0.9176\n",
      "Epoch 116/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2109 - acc: 0.9187 - val_loss: 0.2419 - val_acc: 0.9108\n",
      "Epoch 117/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2117 - acc: 0.9192 - val_loss: 0.2281 - val_acc: 0.9210\n",
      "Epoch 118/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2120 - acc: 0.9200 - val_loss: 0.2263 - val_acc: 0.9206\n",
      "Epoch 119/1000\n",
      "27952/27952 [==============================] - 11s - loss: 0.2140 - acc: 0.9183 - val_loss: 0.2016 - val_acc: 0.9246\n",
      "Epoch 120/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2099 - acc: 0.9196 - val_loss: 0.2572 - val_acc: 0.9066\n",
      "Epoch 121/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2137 - acc: 0.9171 - val_loss: 0.2026 - val_acc: 0.9240\n",
      "Epoch 122/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2103 - acc: 0.9200 - val_loss: 0.1958 - val_acc: 0.9292\n",
      "Epoch 123/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2106 - acc: 0.9179 - val_loss: 0.2143 - val_acc: 0.9181\n",
      "Epoch 124/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.2119 - acc: 0.9182 - val_loss: 0.2033 - val_acc: 0.9272\n",
      "Epoch 125/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27952/27952 [==============================] - 10s - loss: 0.2095 - acc: 0.9183 - val_loss: 0.2011 - val_acc: 0.9262\n",
      "Epoch 126/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2103 - acc: 0.9189 - val_loss: 0.2025 - val_acc: 0.9219\n",
      "Epoch 127/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2114 - acc: 0.9188 - val_loss: 0.2184 - val_acc: 0.9153\n",
      "Epoch 128/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2079 - acc: 0.9207 - val_loss: 0.2033 - val_acc: 0.9276\n",
      "Epoch 129/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2085 - acc: 0.9199 - val_loss: 0.2451 - val_acc: 0.9048\n",
      "Epoch 130/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2093 - acc: 0.9184 - val_loss: 0.1944 - val_acc: 0.9252\n",
      "Epoch 131/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2082 - acc: 0.9197 - val_loss: 0.2252 - val_acc: 0.9204\n",
      "Epoch 132/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2052 - acc: 0.9211 - val_loss: 0.2178 - val_acc: 0.9177\n",
      "Epoch 133/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2053 - acc: 0.9201 - val_loss: 0.2074 - val_acc: 0.9242\n",
      "Epoch 134/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2071 - acc: 0.9220 - val_loss: 0.2372 - val_acc: 0.9087\n",
      "Epoch 135/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2067 - acc: 0.9211 - val_loss: 0.1997 - val_acc: 0.9259\n",
      "Epoch 136/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2083 - acc: 0.9191 - val_loss: 0.2001 - val_acc: 0.9237\n",
      "Epoch 137/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2094 - acc: 0.9201 - val_loss: 0.2267 - val_acc: 0.9164\n",
      "Epoch 138/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.2096 - acc: 0.9201 - val_loss: 0.2013 - val_acc: 0.9266\n",
      "Epoch 139/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2116 - acc: 0.9189 - val_loss: 0.2163 - val_acc: 0.9213\n",
      "Epoch 140/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2088 - acc: 0.9208 - val_loss: 0.1941 - val_acc: 0.9292\n",
      "Epoch 141/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.2129 - acc: 0.9179 - val_loss: 0.1862 - val_acc: 0.9316\n",
      "Epoch 142/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2062 - acc: 0.9201 - val_loss: 0.2077 - val_acc: 0.9219\n",
      "Epoch 143/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2066 - acc: 0.9208 - val_loss: 0.2239 - val_acc: 0.9137\n",
      "Epoch 144/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2056 - acc: 0.9201 - val_loss: 0.2209 - val_acc: 0.9113\n",
      "Epoch 145/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.2030 - acc: 0.9207 - val_loss: 0.2048 - val_acc: 0.9280\n",
      "Epoch 146/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2067 - acc: 0.9199 - val_loss: 0.2095 - val_acc: 0.9257\n",
      "Epoch 147/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.2029 - acc: 0.9214 - val_loss: 0.2209 - val_acc: 0.9167\n",
      "Epoch 148/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.2047 - acc: 0.9205 - val_loss: 0.1959 - val_acc: 0.9293\n",
      "Epoch 149/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2044 - acc: 0.9226 - val_loss: 0.2303 - val_acc: 0.9147\n",
      "Epoch 150/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.2012 - acc: 0.9215 - val_loss: 0.1941 - val_acc: 0.9300\n",
      "Epoch 151/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1986 - acc: 0.9235 - val_loss: 0.2156 - val_acc: 0.9256\n",
      "Epoch 152/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2020 - acc: 0.9232 - val_loss: 0.2105 - val_acc: 0.9242\n",
      "Epoch 153/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2010 - acc: 0.9219 - val_loss: 0.2086 - val_acc: 0.9193\n",
      "Epoch 154/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2011 - acc: 0.9220 - val_loss: 0.1965 - val_acc: 0.9302\n",
      "Epoch 155/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.2001 - acc: 0.9224 - val_loss: 0.1918 - val_acc: 0.9300\n",
      "Epoch 156/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.2006 - acc: 0.9223 - val_loss: 0.1823 - val_acc: 0.9325\n",
      "Epoch 157/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1993 - acc: 0.9218 - val_loss: 0.1859 - val_acc: 0.9303\n",
      "Epoch 158/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1985 - acc: 0.9229 - val_loss: 0.1854 - val_acc: 0.9310\n",
      "Epoch 159/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1955 - acc: 0.9237 - val_loss: 0.2138 - val_acc: 0.9193\n",
      "Epoch 160/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1998 - acc: 0.9232 - val_loss: 0.1874 - val_acc: 0.9319\n",
      "Epoch 161/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1976 - acc: 0.9234 - val_loss: 0.1979 - val_acc: 0.9270\n",
      "Epoch 162/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1985 - acc: 0.9222 - val_loss: 0.2054 - val_acc: 0.9184\n",
      "Epoch 163/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1965 - acc: 0.9239 - val_loss: 0.2585 - val_acc: 0.9108\n",
      "Epoch 164/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1970 - acc: 0.9240 - val_loss: 0.2075 - val_acc: 0.9240\n",
      "Epoch 165/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.1945 - acc: 0.9245 - val_loss: 0.1890 - val_acc: 0.9272\n",
      "Epoch 166/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1974 - acc: 0.9235 - val_loss: 0.1985 - val_acc: 0.9299\n",
      "Epoch 167/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1946 - acc: 0.9243 - val_loss: 0.1997 - val_acc: 0.9216\n",
      "Epoch 168/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1941 - acc: 0.9238 - val_loss: 0.2238 - val_acc: 0.9210\n",
      "Epoch 169/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1933 - acc: 0.9238 - val_loss: 0.1943 - val_acc: 0.9323\n",
      "Epoch 170/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1928 - acc: 0.9246 - val_loss: 0.1796 - val_acc: 0.9357\n",
      "Epoch 171/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1927 - acc: 0.9259 - val_loss: 0.2127 - val_acc: 0.9204\n",
      "Epoch 172/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1941 - acc: 0.9237 - val_loss: 0.2011 - val_acc: 0.9254\n",
      "Epoch 173/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1950 - acc: 0.9247 - val_loss: 0.1950 - val_acc: 0.9290\n",
      "Epoch 174/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1921 - acc: 0.9236 - val_loss: 0.1934 - val_acc: 0.9290\n",
      "Epoch 175/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.1939 - acc: 0.9243 - val_loss: 0.1950 - val_acc: 0.9283\n",
      "Epoch 176/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1917 - acc: 0.9255 - val_loss: 0.2289 - val_acc: 0.9067\n",
      "Epoch 177/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1886 - acc: 0.9273 - val_loss: 0.1769 - val_acc: 0.9332\n",
      "Epoch 178/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1918 - acc: 0.9240 - val_loss: 0.1791 - val_acc: 0.9356\n",
      "Epoch 179/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1917 - acc: 0.9261 - val_loss: 0.1767 - val_acc: 0.9336\n",
      "Epoch 180/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1903 - acc: 0.9257 - val_loss: 0.2144 - val_acc: 0.9180\n",
      "Epoch 181/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1913 - acc: 0.9259 - val_loss: 0.1747 - val_acc: 0.9329\n",
      "Epoch 182/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1899 - acc: 0.9257 - val_loss: 0.2282 - val_acc: 0.9169\n",
      "Epoch 183/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1878 - acc: 0.9276 - val_loss: 0.1876 - val_acc: 0.9320\n",
      "Epoch 184/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.1871 - acc: 0.9259 - val_loss: 0.2411 - val_acc: 0.9144\n",
      "Epoch 185/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1898 - acc: 0.9260 - val_loss: 0.1861 - val_acc: 0.9332\n",
      "Epoch 186/1000\n",
      "27952/27952 [==============================] - 12s - loss: 0.1885 - acc: 0.9268 - val_loss: 0.2052 - val_acc: 0.9269\n",
      "Epoch 187/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1854 - acc: 0.9282 - val_loss: 0.2568 - val_acc: 0.9080\n",
      "Epoch 188/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1880 - acc: 0.9268 - val_loss: 0.2123 - val_acc: 0.9216\n",
      "Epoch 189/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1854 - acc: 0.9259 - val_loss: 0.1773 - val_acc: 0.9357\n",
      "Epoch 190/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1859 - acc: 0.9268 - val_loss: 0.1737 - val_acc: 0.9360\n",
      "Epoch 191/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1847 - acc: 0.9278 - val_loss: 0.1781 - val_acc: 0.9360\n",
      "Epoch 192/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1869 - acc: 0.9257 - val_loss: 0.1745 - val_acc: 0.9315\n",
      "Epoch 193/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1886 - acc: 0.9268 - val_loss: 0.1962 - val_acc: 0.9206\n",
      "Epoch 194/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1846 - acc: 0.9270 - val_loss: 0.1874 - val_acc: 0.9349\n",
      "Epoch 195/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1885 - acc: 0.9271 - val_loss: 0.1814 - val_acc: 0.9327\n",
      "Epoch 196/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1860 - acc: 0.9263 - val_loss: 0.2387 - val_acc: 0.9081\n",
      "Epoch 197/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1878 - acc: 0.9266 - val_loss: 0.2223 - val_acc: 0.9139\n",
      "Epoch 198/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1872 - acc: 0.9265 - val_loss: 0.1961 - val_acc: 0.9270\n",
      "Epoch 199/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1831 - acc: 0.9289 - val_loss: 0.1942 - val_acc: 0.9297\n",
      "Epoch 200/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1875 - acc: 0.9269 - val_loss: 0.1850 - val_acc: 0.9270\n",
      "Epoch 201/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1848 - acc: 0.9256 - val_loss: 0.1770 - val_acc: 0.9336\n",
      "Epoch 202/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1860 - acc: 0.9289 - val_loss: 0.2160 - val_acc: 0.9114\n",
      "Epoch 203/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1845 - acc: 0.9283 - val_loss: 0.1965 - val_acc: 0.9330\n",
      "Epoch 204/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1835 - acc: 0.9264 - val_loss: 0.1753 - val_acc: 0.9359\n",
      "Epoch 205/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.1815 - acc: 0.9276 - val_loss: 0.1924 - val_acc: 0.9254\n",
      "Epoch 206/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.1832 - acc: 0.9276 - val_loss: 0.1767 - val_acc: 0.9322\n",
      "Epoch 207/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.1823 - acc: 0.9281 - val_loss: 0.2111 - val_acc: 0.9187\n",
      "Epoch 208/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1839 - acc: 0.9274 - val_loss: 0.1897 - val_acc: 0.9273\n",
      "Epoch 209/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1814 - acc: 0.9288 - val_loss: 0.2085 - val_acc: 0.9230\n",
      "Epoch 210/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1815 - acc: 0.9284 - val_loss: 0.1899 - val_acc: 0.9289\n",
      "Epoch 211/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1842 - acc: 0.9273 - val_loss: 0.2316 - val_acc: 0.9139\n",
      "Epoch 212/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1795 - acc: 0.9295 - val_loss: 0.1799 - val_acc: 0.9320\n",
      "Epoch 213/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1812 - acc: 0.9293 - val_loss: 0.1839 - val_acc: 0.9263\n",
      "Epoch 214/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1775 - acc: 0.9295 - val_loss: 0.1726 - val_acc: 0.9399\n",
      "Epoch 215/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1784 - acc: 0.9284 - val_loss: 0.1675 - val_acc: 0.9396\n",
      "Epoch 216/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1798 - acc: 0.9310 - val_loss: 0.1902 - val_acc: 0.9262\n",
      "Epoch 217/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1818 - acc: 0.9284 - val_loss: 0.1759 - val_acc: 0.9347\n",
      "Epoch 218/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1759 - acc: 0.9311 - val_loss: 0.1858 - val_acc: 0.9345\n",
      "Epoch 219/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1785 - acc: 0.9300 - val_loss: 0.1741 - val_acc: 0.9390\n",
      "Epoch 220/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1778 - acc: 0.9304 - val_loss: 0.1725 - val_acc: 0.9373\n",
      "Epoch 221/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1744 - acc: 0.9305 - val_loss: 0.2453 - val_acc: 0.9124\n",
      "Epoch 222/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1826 - acc: 0.9279 - val_loss: 0.1788 - val_acc: 0.9326\n",
      "Epoch 223/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1776 - acc: 0.9306 - val_loss: 0.2034 - val_acc: 0.9214\n",
      "Epoch 224/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1764 - acc: 0.9308 - val_loss: 0.1762 - val_acc: 0.9375\n",
      "Epoch 225/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1783 - acc: 0.9299 - val_loss: 0.1877 - val_acc: 0.9295\n",
      "Epoch 226/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1766 - acc: 0.9298 - val_loss: 0.2160 - val_acc: 0.9206\n",
      "Epoch 227/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1798 - acc: 0.9288 - val_loss: 0.1811 - val_acc: 0.9337\n",
      "Epoch 228/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.1782 - acc: 0.9295 - val_loss: 0.2189 - val_acc: 0.9097\n",
      "Epoch 229/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1734 - acc: 0.9306 - val_loss: 0.1848 - val_acc: 0.9320\n",
      "Epoch 230/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1756 - acc: 0.9294 - val_loss: 0.2080 - val_acc: 0.9207\n",
      "Epoch 231/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1765 - acc: 0.9295 - val_loss: 0.2170 - val_acc: 0.9194\n",
      "Epoch 232/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1749 - acc: 0.9305 - val_loss: 0.2024 - val_acc: 0.9196\n",
      "Epoch 233/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1764 - acc: 0.9297 - val_loss: 0.1881 - val_acc: 0.9319\n",
      "Epoch 234/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1733 - acc: 0.9316 - val_loss: 0.1645 - val_acc: 0.9388\n",
      "Epoch 235/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1752 - acc: 0.9321 - val_loss: 0.1710 - val_acc: 0.9370\n",
      "Epoch 236/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1742 - acc: 0.9298 - val_loss: 0.1937 - val_acc: 0.9277\n",
      "Epoch 237/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1733 - acc: 0.9312 - val_loss: 0.1719 - val_acc: 0.9319\n",
      "Epoch 238/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1724 - acc: 0.9313 - val_loss: 0.1614 - val_acc: 0.9390\n",
      "Epoch 239/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1722 - acc: 0.9315 - val_loss: 0.1628 - val_acc: 0.9390\n",
      "Epoch 240/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.1718 - acc: 0.9320 - val_loss: 0.1670 - val_acc: 0.9333\n",
      "Epoch 241/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.1719 - acc: 0.9317 - val_loss: 0.1819 - val_acc: 0.9322\n",
      "Epoch 242/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1731 - acc: 0.9301 - val_loss: 0.1655 - val_acc: 0.9365\n",
      "Epoch 243/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1747 - acc: 0.9324 - val_loss: 0.1699 - val_acc: 0.9359\n",
      "Epoch 244/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1720 - acc: 0.9318 - val_loss: 0.1854 - val_acc: 0.9312\n",
      "Epoch 245/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1725 - acc: 0.9320 - val_loss: 0.1745 - val_acc: 0.9362\n",
      "Epoch 246/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1708 - acc: 0.9326 - val_loss: 0.1773 - val_acc: 0.9356\n",
      "Epoch 247/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1728 - acc: 0.9321 - val_loss: 0.1990 - val_acc: 0.9214\n",
      "Epoch 248/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1713 - acc: 0.9316 - val_loss: 0.2004 - val_acc: 0.9246\n",
      "Epoch 249/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27952/27952 [==============================] - 5s - loss: 0.1739 - acc: 0.9301 - val_loss: 0.1785 - val_acc: 0.9325\n",
      "Epoch 250/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1683 - acc: 0.9331 - val_loss: 0.1630 - val_acc: 0.9389\n",
      "Epoch 251/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1684 - acc: 0.9343 - val_loss: 0.1993 - val_acc: 0.9266\n",
      "Epoch 252/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1702 - acc: 0.9323 - val_loss: 0.1737 - val_acc: 0.9355\n",
      "Epoch 253/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1750 - acc: 0.9295 - val_loss: 0.1920 - val_acc: 0.9277\n",
      "Epoch 254/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1745 - acc: 0.9305 - val_loss: 0.1804 - val_acc: 0.9300\n",
      "Epoch 255/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1702 - acc: 0.9337 - val_loss: 0.1575 - val_acc: 0.9430\n",
      "Epoch 256/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1667 - acc: 0.9337 - val_loss: 0.1856 - val_acc: 0.9312\n",
      "Epoch 257/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1663 - acc: 0.9342 - val_loss: 0.1713 - val_acc: 0.9388\n",
      "Epoch 258/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1687 - acc: 0.9344 - val_loss: 0.1783 - val_acc: 0.9376\n",
      "Epoch 259/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1678 - acc: 0.9335 - val_loss: 0.1799 - val_acc: 0.9313\n",
      "Epoch 260/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1656 - acc: 0.9341 - val_loss: 0.1554 - val_acc: 0.9423\n",
      "Epoch 261/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1651 - acc: 0.9339 - val_loss: 0.1679 - val_acc: 0.9452\n",
      "Epoch 262/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1665 - acc: 0.9337 - val_loss: 0.1802 - val_acc: 0.9340\n",
      "Epoch 263/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1653 - acc: 0.9346 - val_loss: 0.1687 - val_acc: 0.9390\n",
      "Epoch 264/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1656 - acc: 0.9343 - val_loss: 0.1749 - val_acc: 0.9365\n",
      "Epoch 265/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1654 - acc: 0.9341 - val_loss: 0.1633 - val_acc: 0.9408\n",
      "Epoch 266/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1663 - acc: 0.9352 - val_loss: 0.1709 - val_acc: 0.9379\n",
      "Epoch 267/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1647 - acc: 0.9336 - val_loss: 0.2359 - val_acc: 0.9199\n",
      "Epoch 268/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1641 - acc: 0.9337 - val_loss: 0.1671 - val_acc: 0.9367\n",
      "Epoch 269/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1649 - acc: 0.9345 - val_loss: 0.1515 - val_acc: 0.9428\n",
      "Epoch 270/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1692 - acc: 0.9317 - val_loss: 0.2129 - val_acc: 0.9247\n",
      "Epoch 271/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1662 - acc: 0.9357 - val_loss: 0.1637 - val_acc: 0.9405\n",
      "Epoch 272/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1664 - acc: 0.9337 - val_loss: 0.1616 - val_acc: 0.9386\n",
      "Epoch 273/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1636 - acc: 0.9359 - val_loss: 0.1628 - val_acc: 0.9418\n",
      "Epoch 274/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1641 - acc: 0.9333 - val_loss: 0.2129 - val_acc: 0.9173\n",
      "Epoch 275/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1655 - acc: 0.9347 - val_loss: 0.1783 - val_acc: 0.9345\n",
      "Epoch 276/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1646 - acc: 0.9333 - val_loss: 0.1702 - val_acc: 0.9362\n",
      "Epoch 277/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1633 - acc: 0.9355 - val_loss: 0.2575 - val_acc: 0.8931\n",
      "Epoch 278/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1614 - acc: 0.9368 - val_loss: 0.1670 - val_acc: 0.9367\n",
      "Epoch 279/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1614 - acc: 0.9364 - val_loss: 0.2132 - val_acc: 0.9180\n",
      "Epoch 280/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1646 - acc: 0.9332 - val_loss: 0.2259 - val_acc: 0.9174\n",
      "Epoch 281/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1650 - acc: 0.9336 - val_loss: 0.1603 - val_acc: 0.9452\n",
      "Epoch 282/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1614 - acc: 0.9364 - val_loss: 0.1881 - val_acc: 0.9287\n",
      "Epoch 283/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1619 - acc: 0.9351 - val_loss: 0.2103 - val_acc: 0.9244\n",
      "Epoch 284/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1613 - acc: 0.9339 - val_loss: 0.1834 - val_acc: 0.9279\n",
      "Epoch 285/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1602 - acc: 0.9357 - val_loss: 0.1600 - val_acc: 0.9416\n",
      "Epoch 286/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1596 - acc: 0.9360 - val_loss: 0.1747 - val_acc: 0.9319\n",
      "Epoch 287/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1597 - acc: 0.9357 - val_loss: 0.1555 - val_acc: 0.9443\n",
      "Epoch 288/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1593 - acc: 0.9358 - val_loss: 0.1495 - val_acc: 0.9438\n",
      "Epoch 289/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1604 - acc: 0.9363 - val_loss: 0.1942 - val_acc: 0.9253\n",
      "Epoch 290/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1589 - acc: 0.9366 - val_loss: 0.1638 - val_acc: 0.9418\n",
      "Epoch 291/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1592 - acc: 0.9352 - val_loss: 0.1741 - val_acc: 0.9373\n",
      "Epoch 292/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1638 - acc: 0.9359 - val_loss: 0.2028 - val_acc: 0.9246\n",
      "Epoch 293/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1589 - acc: 0.9358 - val_loss: 0.1604 - val_acc: 0.9403\n",
      "Epoch 294/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1590 - acc: 0.9361 - val_loss: 0.1979 - val_acc: 0.9269\n",
      "Epoch 295/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1591 - acc: 0.9346 - val_loss: 0.1665 - val_acc: 0.9386\n",
      "Epoch 296/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1584 - acc: 0.9350 - val_loss: 0.2607 - val_acc: 0.8924\n",
      "Epoch 297/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1581 - acc: 0.9368 - val_loss: 0.1835 - val_acc: 0.9347\n",
      "Epoch 298/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1614 - acc: 0.9355 - val_loss: 0.2327 - val_acc: 0.9194\n",
      "Epoch 299/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1567 - acc: 0.9369 - val_loss: 0.1608 - val_acc: 0.9426\n",
      "Epoch 300/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1577 - acc: 0.9369 - val_loss: 0.1598 - val_acc: 0.9415\n",
      "Epoch 301/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1568 - acc: 0.9379 - val_loss: 0.1778 - val_acc: 0.9362\n",
      "Epoch 302/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1579 - acc: 0.9370 - val_loss: 0.1667 - val_acc: 0.9386\n",
      "Epoch 303/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1586 - acc: 0.9362 - val_loss: 0.2085 - val_acc: 0.9194\n",
      "Epoch 304/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1588 - acc: 0.9366 - val_loss: 0.1595 - val_acc: 0.9403\n",
      "Epoch 305/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1570 - acc: 0.9380 - val_loss: 0.1982 - val_acc: 0.9306\n",
      "Epoch 306/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1541 - acc: 0.9379 - val_loss: 0.1538 - val_acc: 0.9418\n",
      "Epoch 307/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1562 - acc: 0.9373 - val_loss: 0.1605 - val_acc: 0.9388\n",
      "Epoch 308/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1562 - acc: 0.9379 - val_loss: 0.1707 - val_acc: 0.9376\n",
      "Epoch 309/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1554 - acc: 0.9369 - val_loss: 0.1526 - val_acc: 0.9449\n",
      "Epoch 310/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1556 - acc: 0.9372 - val_loss: 0.1698 - val_acc: 0.9350\n",
      "Epoch 311/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1545 - acc: 0.9391 - val_loss: 0.1583 - val_acc: 0.9458\n",
      "Epoch 312/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1532 - acc: 0.9377 - val_loss: 0.2314 - val_acc: 0.9229\n",
      "Epoch 313/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1562 - acc: 0.9371 - val_loss: 0.1672 - val_acc: 0.9389\n",
      "Epoch 314/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1571 - acc: 0.9372 - val_loss: 0.1634 - val_acc: 0.9347\n",
      "Epoch 315/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1553 - acc: 0.9374 - val_loss: 0.1656 - val_acc: 0.9373\n",
      "Epoch 316/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1550 - acc: 0.9381 - val_loss: 0.1610 - val_acc: 0.9423\n",
      "Epoch 317/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1506 - acc: 0.9391 - val_loss: 0.1792 - val_acc: 0.9317\n",
      "Epoch 318/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1528 - acc: 0.9383 - val_loss: 0.1777 - val_acc: 0.9310\n",
      "Epoch 319/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1540 - acc: 0.9393 - val_loss: 0.1594 - val_acc: 0.9406\n",
      "Epoch 320/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1551 - acc: 0.9370 - val_loss: 0.1899 - val_acc: 0.9380\n",
      "Epoch 321/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1546 - acc: 0.9377 - val_loss: 0.1591 - val_acc: 0.9395\n",
      "Epoch 322/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1550 - acc: 0.9377 - val_loss: 0.1681 - val_acc: 0.9395\n",
      "Epoch 323/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1513 - acc: 0.9386 - val_loss: 0.1804 - val_acc: 0.9317\n",
      "Epoch 324/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1537 - acc: 0.9384 - val_loss: 0.1608 - val_acc: 0.9409\n",
      "Epoch 325/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1531 - acc: 0.9398 - val_loss: 0.1901 - val_acc: 0.9286\n",
      "Epoch 326/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1516 - acc: 0.9390 - val_loss: 0.1463 - val_acc: 0.9455\n",
      "Epoch 327/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1518 - acc: 0.9393 - val_loss: 0.2028 - val_acc: 0.9157\n",
      "Epoch 328/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1537 - acc: 0.9390 - val_loss: 0.1896 - val_acc: 0.9257\n",
      "Epoch 329/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1503 - acc: 0.9404 - val_loss: 0.1568 - val_acc: 0.9400\n",
      "Epoch 330/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1508 - acc: 0.9395 - val_loss: 0.1713 - val_acc: 0.9367\n",
      "Epoch 331/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1501 - acc: 0.9410 - val_loss: 0.1457 - val_acc: 0.9469\n",
      "Epoch 332/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1513 - acc: 0.9396 - val_loss: 0.1491 - val_acc: 0.9442\n",
      "Epoch 333/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1482 - acc: 0.9398 - val_loss: 0.1572 - val_acc: 0.9426\n",
      "Epoch 334/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1513 - acc: 0.9392 - val_loss: 0.1598 - val_acc: 0.9346\n",
      "Epoch 335/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1492 - acc: 0.9410 - val_loss: 0.1838 - val_acc: 0.9316\n",
      "Epoch 336/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1472 - acc: 0.9401 - val_loss: 0.1773 - val_acc: 0.9386\n",
      "Epoch 337/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1520 - acc: 0.9390 - val_loss: 0.1654 - val_acc: 0.9365\n",
      "Epoch 338/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1488 - acc: 0.9391 - val_loss: 0.1594 - val_acc: 0.9429\n",
      "Epoch 339/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1516 - acc: 0.9398 - val_loss: 0.1741 - val_acc: 0.9352\n",
      "Epoch 340/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1482 - acc: 0.9403 - val_loss: 0.1771 - val_acc: 0.9382\n",
      "Epoch 341/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1490 - acc: 0.9391 - val_loss: 0.1460 - val_acc: 0.9461\n",
      "Epoch 342/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1500 - acc: 0.9392 - val_loss: 0.1629 - val_acc: 0.9380\n",
      "Epoch 343/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1484 - acc: 0.9407 - val_loss: 0.1429 - val_acc: 0.9495\n",
      "Epoch 344/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1458 - acc: 0.9413 - val_loss: 0.1967 - val_acc: 0.9270\n",
      "Epoch 345/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1524 - acc: 0.9382 - val_loss: 0.1791 - val_acc: 0.9335\n",
      "Epoch 346/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1468 - acc: 0.9411 - val_loss: 0.1643 - val_acc: 0.9406\n",
      "Epoch 347/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1517 - acc: 0.9393 - val_loss: 0.1698 - val_acc: 0.9405\n",
      "Epoch 348/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1480 - acc: 0.9409 - val_loss: 0.1546 - val_acc: 0.9415\n",
      "Epoch 349/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1496 - acc: 0.9399 - val_loss: 0.1917 - val_acc: 0.9317\n",
      "Epoch 350/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1415 - acc: 0.9443 - val_loss: 0.2255 - val_acc: 0.9217\n",
      "Epoch 351/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1455 - acc: 0.9428 - val_loss: 0.2004 - val_acc: 0.9213\n",
      "Epoch 352/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1472 - acc: 0.9415 - val_loss: 0.1498 - val_acc: 0.9445\n",
      "Epoch 353/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1464 - acc: 0.9419 - val_loss: 0.1781 - val_acc: 0.9339\n",
      "Epoch 354/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1477 - acc: 0.9411 - val_loss: 0.1664 - val_acc: 0.9336\n",
      "Epoch 355/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1472 - acc: 0.9396 - val_loss: 0.1418 - val_acc: 0.9488\n",
      "Epoch 356/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1483 - acc: 0.9397 - val_loss: 0.1462 - val_acc: 0.9443\n",
      "Epoch 357/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1456 - acc: 0.9409 - val_loss: 0.1544 - val_acc: 0.9463\n",
      "Epoch 358/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1494 - acc: 0.9401 - val_loss: 0.1668 - val_acc: 0.9385\n",
      "Epoch 359/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1480 - acc: 0.9403 - val_loss: 0.1577 - val_acc: 0.9412\n",
      "Epoch 360/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1479 - acc: 0.9400 - val_loss: 0.1632 - val_acc: 0.9375\n",
      "Epoch 361/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1467 - acc: 0.9412 - val_loss: 0.1547 - val_acc: 0.9402\n",
      "Epoch 362/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1437 - acc: 0.9411 - val_loss: 0.1592 - val_acc: 0.9392\n",
      "Epoch 363/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1472 - acc: 0.9396 - val_loss: 0.1748 - val_acc: 0.9362\n",
      "Epoch 364/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1466 - acc: 0.9415 - val_loss: 0.1891 - val_acc: 0.9292\n",
      "Epoch 365/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1447 - acc: 0.9414 - val_loss: 0.1552 - val_acc: 0.9438\n",
      "Epoch 366/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1454 - acc: 0.9407 - val_loss: 0.1622 - val_acc: 0.9395\n",
      "Epoch 367/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1465 - acc: 0.9405 - val_loss: 0.1697 - val_acc: 0.9449\n",
      "Epoch 368/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1449 - acc: 0.9426 - val_loss: 0.1629 - val_acc: 0.9390\n",
      "Epoch 369/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1467 - acc: 0.9418 - val_loss: 0.1599 - val_acc: 0.9405\n",
      "Epoch 370/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1440 - acc: 0.9418 - val_loss: 0.1728 - val_acc: 0.9326\n",
      "Epoch 371/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1426 - acc: 0.9424 - val_loss: 0.1594 - val_acc: 0.9436\n",
      "Epoch 372/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1434 - acc: 0.9412 - val_loss: 0.1635 - val_acc: 0.9396\n",
      "Epoch 373/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27952/27952 [==============================] - 6s - loss: 0.1454 - acc: 0.9401 - val_loss: 0.2037 - val_acc: 0.9282\n",
      "Epoch 374/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1463 - acc: 0.9412 - val_loss: 0.1395 - val_acc: 0.9493\n",
      "Epoch 375/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1449 - acc: 0.9405 - val_loss: 0.1455 - val_acc: 0.9471\n",
      "Epoch 376/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1430 - acc: 0.9426 - val_loss: 0.1586 - val_acc: 0.9419\n",
      "Epoch 377/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1424 - acc: 0.9413 - val_loss: 0.1509 - val_acc: 0.9438\n",
      "Epoch 378/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1438 - acc: 0.9425 - val_loss: 0.1587 - val_acc: 0.9410\n",
      "Epoch 379/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1421 - acc: 0.9439 - val_loss: 0.1512 - val_acc: 0.9442\n",
      "Epoch 380/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1432 - acc: 0.9416 - val_loss: 0.1569 - val_acc: 0.9429\n",
      "Epoch 381/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1425 - acc: 0.9423 - val_loss: 0.1610 - val_acc: 0.9436\n",
      "Epoch 382/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1445 - acc: 0.9413 - val_loss: 0.1638 - val_acc: 0.9398\n",
      "Epoch 383/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1427 - acc: 0.9418 - val_loss: 0.1606 - val_acc: 0.9416\n",
      "Epoch 384/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1433 - acc: 0.9412 - val_loss: 0.1420 - val_acc: 0.9466\n",
      "Epoch 385/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1451 - acc: 0.9418 - val_loss: 0.1802 - val_acc: 0.9360\n",
      "Epoch 386/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1440 - acc: 0.9411 - val_loss: 0.1681 - val_acc: 0.9408\n",
      "Epoch 387/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1441 - acc: 0.9411 - val_loss: 0.1510 - val_acc: 0.9409\n",
      "Epoch 388/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1437 - acc: 0.9433 - val_loss: 0.1870 - val_acc: 0.9336\n",
      "Epoch 389/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1420 - acc: 0.9434 - val_loss: 0.1565 - val_acc: 0.9436\n",
      "Epoch 390/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1425 - acc: 0.9415 - val_loss: 0.1970 - val_acc: 0.9203\n",
      "Epoch 391/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1427 - acc: 0.9415 - val_loss: 0.1464 - val_acc: 0.9465\n",
      "Epoch 392/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1416 - acc: 0.9424 - val_loss: 0.1606 - val_acc: 0.9419\n",
      "Epoch 393/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1391 - acc: 0.9433 - val_loss: 0.1604 - val_acc: 0.9410\n",
      "Epoch 394/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1411 - acc: 0.9433 - val_loss: 0.1838 - val_acc: 0.9305\n",
      "Epoch 395/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1437 - acc: 0.9432 - val_loss: 0.1472 - val_acc: 0.9479\n",
      "Epoch 396/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1410 - acc: 0.9424 - val_loss: 0.1493 - val_acc: 0.9461\n",
      "Epoch 397/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1414 - acc: 0.9415 - val_loss: 0.1619 - val_acc: 0.9406\n",
      "Epoch 398/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1396 - acc: 0.9429 - val_loss: 0.1621 - val_acc: 0.9408\n",
      "Epoch 399/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1433 - acc: 0.9422 - val_loss: 0.1469 - val_acc: 0.9438\n",
      "Epoch 400/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1441 - acc: 0.9416 - val_loss: 0.1514 - val_acc: 0.9428\n",
      "Epoch 401/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1386 - acc: 0.9436 - val_loss: 0.1625 - val_acc: 0.9408\n",
      "Epoch 402/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1398 - acc: 0.9435 - val_loss: 0.1621 - val_acc: 0.9409\n",
      "Epoch 403/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1444 - acc: 0.9411 - val_loss: 0.1682 - val_acc: 0.9369\n",
      "Epoch 404/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1369 - acc: 0.9454 - val_loss: 0.1945 - val_acc: 0.9257\n",
      "Epoch 405/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1413 - acc: 0.9428 - val_loss: 0.1619 - val_acc: 0.9395\n",
      "Epoch 406/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1375 - acc: 0.9438 - val_loss: 0.1418 - val_acc: 0.9493\n",
      "Epoch 407/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1391 - acc: 0.9440 - val_loss: 0.1488 - val_acc: 0.9455\n",
      "Epoch 408/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1385 - acc: 0.9442 - val_loss: 0.1522 - val_acc: 0.9449\n",
      "Epoch 409/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1416 - acc: 0.9420 - val_loss: 0.1527 - val_acc: 0.9440\n",
      "Epoch 410/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1407 - acc: 0.9433 - val_loss: 0.1427 - val_acc: 0.9492\n",
      "Epoch 411/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1392 - acc: 0.9432 - val_loss: 0.1687 - val_acc: 0.9367\n",
      "Epoch 412/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1419 - acc: 0.9419 - val_loss: 0.1632 - val_acc: 0.9393\n",
      "Epoch 413/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1409 - acc: 0.9420 - val_loss: 0.1581 - val_acc: 0.9410\n",
      "Epoch 414/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1377 - acc: 0.9439 - val_loss: 0.1792 - val_acc: 0.9357\n",
      "Epoch 415/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1355 - acc: 0.9443 - val_loss: 0.1627 - val_acc: 0.9425\n",
      "Epoch 416/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1372 - acc: 0.9455 - val_loss: 0.1496 - val_acc: 0.9472\n",
      "Epoch 417/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1382 - acc: 0.9461 - val_loss: 0.1453 - val_acc: 0.9445\n",
      "Epoch 418/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1426 - acc: 0.9429 - val_loss: 0.1591 - val_acc: 0.9403\n",
      "Epoch 419/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1391 - acc: 0.9443 - val_loss: 0.1520 - val_acc: 0.9436\n",
      "Epoch 420/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1422 - acc: 0.9414 - val_loss: 0.1644 - val_acc: 0.9393\n",
      "Epoch 421/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1377 - acc: 0.9441 - val_loss: 0.1528 - val_acc: 0.9426\n",
      "Epoch 422/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1377 - acc: 0.9439 - val_loss: 0.1404 - val_acc: 0.9468\n",
      "Epoch 423/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1366 - acc: 0.9442 - val_loss: 0.1714 - val_acc: 0.9365\n",
      "Epoch 424/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1346 - acc: 0.9467 - val_loss: 0.1723 - val_acc: 0.9372\n",
      "Epoch 425/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1370 - acc: 0.9446 - val_loss: 0.1875 - val_acc: 0.9277\n",
      "Epoch 426/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1395 - acc: 0.9445 - val_loss: 0.1535 - val_acc: 0.9425\n",
      "Epoch 427/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1393 - acc: 0.9453 - val_loss: 0.1518 - val_acc: 0.9420\n",
      "Epoch 428/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1394 - acc: 0.9442 - val_loss: 0.1531 - val_acc: 0.9439\n",
      "Epoch 429/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1370 - acc: 0.9445 - val_loss: 0.1451 - val_acc: 0.9481\n",
      "Epoch 430/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1375 - acc: 0.9445 - val_loss: 0.1861 - val_acc: 0.9277\n",
      "Epoch 431/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1370 - acc: 0.9444 - val_loss: 0.1788 - val_acc: 0.9352\n",
      "Epoch 432/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1367 - acc: 0.9448 - val_loss: 0.1524 - val_acc: 0.9450\n",
      "Epoch 433/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1395 - acc: 0.9435 - val_loss: 0.1543 - val_acc: 0.9461\n",
      "Epoch 434/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1390 - acc: 0.9424 - val_loss: 0.1791 - val_acc: 0.9316\n",
      "Epoch 435/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1323 - acc: 0.9465 - val_loss: 0.1592 - val_acc: 0.9383\n",
      "Epoch 436/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1370 - acc: 0.9447 - val_loss: 0.1523 - val_acc: 0.9465\n",
      "Epoch 437/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1354 - acc: 0.9448 - val_loss: 0.1553 - val_acc: 0.9413\n",
      "Epoch 438/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1400 - acc: 0.9437 - val_loss: 0.1462 - val_acc: 0.9488\n",
      "Epoch 439/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1382 - acc: 0.9433 - val_loss: 0.1427 - val_acc: 0.9499\n",
      "Epoch 440/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1354 - acc: 0.9447 - val_loss: 0.1453 - val_acc: 0.9443\n",
      "Epoch 441/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1355 - acc: 0.9451 - val_loss: 0.1500 - val_acc: 0.9439\n",
      "Epoch 442/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1372 - acc: 0.9440 - val_loss: 0.1418 - val_acc: 0.9491\n",
      "Epoch 443/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1345 - acc: 0.9453 - val_loss: 0.1722 - val_acc: 0.9412\n",
      "Epoch 444/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1334 - acc: 0.9459 - val_loss: 0.1545 - val_acc: 0.9448\n",
      "Epoch 445/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1338 - acc: 0.9465 - val_loss: 0.1452 - val_acc: 0.9485\n",
      "Epoch 446/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1361 - acc: 0.9460 - val_loss: 0.1388 - val_acc: 0.9506\n",
      "Epoch 447/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1337 - acc: 0.9464 - val_loss: 0.1545 - val_acc: 0.9403\n",
      "Epoch 448/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1329 - acc: 0.9456 - val_loss: 0.1433 - val_acc: 0.9499\n",
      "Epoch 449/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1330 - acc: 0.9449 - val_loss: 0.1479 - val_acc: 0.9471\n",
      "Epoch 450/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1362 - acc: 0.9458 - val_loss: 0.1611 - val_acc: 0.9416\n",
      "Epoch 451/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1359 - acc: 0.9438 - val_loss: 0.1729 - val_acc: 0.9345\n",
      "Epoch 452/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1343 - acc: 0.9462 - val_loss: 0.1435 - val_acc: 0.9501\n",
      "Epoch 453/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1322 - acc: 0.9468 - val_loss: 0.1510 - val_acc: 0.9452\n",
      "Epoch 454/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1358 - acc: 0.9454 - val_loss: 0.1397 - val_acc: 0.9502\n",
      "Epoch 455/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1353 - acc: 0.9453 - val_loss: 0.1562 - val_acc: 0.9446\n",
      "Epoch 456/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1349 - acc: 0.9459 - val_loss: 0.1502 - val_acc: 0.9476\n",
      "Epoch 457/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1365 - acc: 0.9451 - val_loss: 0.1819 - val_acc: 0.9326\n",
      "Epoch 458/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1353 - acc: 0.9457 - val_loss: 0.1539 - val_acc: 0.9388\n",
      "Epoch 459/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1337 - acc: 0.9461 - val_loss: 0.1478 - val_acc: 0.9469\n",
      "Epoch 460/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1331 - acc: 0.9461 - val_loss: 0.1836 - val_acc: 0.9290\n",
      "Epoch 461/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1347 - acc: 0.9451 - val_loss: 0.1584 - val_acc: 0.9436\n",
      "Epoch 462/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1325 - acc: 0.9454 - val_loss: 0.1907 - val_acc: 0.9299\n",
      "Epoch 463/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1392 - acc: 0.9432 - val_loss: 0.1683 - val_acc: 0.9390\n",
      "Epoch 464/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1343 - acc: 0.9471 - val_loss: 0.1379 - val_acc: 0.9505\n",
      "Epoch 465/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1301 - acc: 0.9478 - val_loss: 0.1395 - val_acc: 0.9482\n",
      "Epoch 466/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1331 - acc: 0.9457 - val_loss: 0.1363 - val_acc: 0.9532\n",
      "Epoch 467/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1330 - acc: 0.9458 - val_loss: 0.1674 - val_acc: 0.9362\n",
      "Epoch 468/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1321 - acc: 0.9467 - val_loss: 0.1553 - val_acc: 0.9446\n",
      "Epoch 469/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1328 - acc: 0.9481 - val_loss: 0.1641 - val_acc: 0.9413\n",
      "Epoch 470/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1353 - acc: 0.9457 - val_loss: 0.1778 - val_acc: 0.9365\n",
      "Epoch 471/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1332 - acc: 0.9466 - val_loss: 0.1448 - val_acc: 0.9505\n",
      "Epoch 472/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1333 - acc: 0.9450 - val_loss: 0.1894 - val_acc: 0.9302\n",
      "Epoch 473/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1331 - acc: 0.9468 - val_loss: 0.1587 - val_acc: 0.9423\n",
      "Epoch 474/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1313 - acc: 0.9463 - val_loss: 0.1423 - val_acc: 0.9461\n",
      "Epoch 475/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1311 - acc: 0.9472 - val_loss: 0.1585 - val_acc: 0.9432\n",
      "Epoch 476/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1310 - acc: 0.9475 - val_loss: 0.1623 - val_acc: 0.9450\n",
      "Epoch 477/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1331 - acc: 0.9459 - val_loss: 0.1581 - val_acc: 0.9440\n",
      "Epoch 478/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1317 - acc: 0.9458 - val_loss: 0.1565 - val_acc: 0.9469\n",
      "Epoch 479/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1358 - acc: 0.9466 - val_loss: 0.1391 - val_acc: 0.9485\n",
      "Epoch 480/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1320 - acc: 0.9446 - val_loss: 0.1423 - val_acc: 0.9499\n",
      "Epoch 481/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1356 - acc: 0.9442 - val_loss: 0.1874 - val_acc: 0.9337\n",
      "Epoch 482/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1312 - acc: 0.9454 - val_loss: 0.1465 - val_acc: 0.9463\n",
      "Epoch 483/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1364 - acc: 0.9453 - val_loss: 0.1497 - val_acc: 0.9478\n",
      "Epoch 484/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1310 - acc: 0.9468 - val_loss: 0.1505 - val_acc: 0.9488\n",
      "Epoch 485/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1330 - acc: 0.9449 - val_loss: 0.1580 - val_acc: 0.9459\n",
      "Epoch 486/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1302 - acc: 0.9482 - val_loss: 0.1434 - val_acc: 0.9481\n",
      "Epoch 487/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1331 - acc: 0.9468 - val_loss: 0.1524 - val_acc: 0.9425\n",
      "Epoch 488/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1293 - acc: 0.9467 - val_loss: 0.1455 - val_acc: 0.9465\n",
      "Epoch 489/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1316 - acc: 0.9464 - val_loss: 0.1946 - val_acc: 0.9226\n",
      "Epoch 490/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1323 - acc: 0.9445 - val_loss: 0.1624 - val_acc: 0.9416\n",
      "Epoch 491/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1308 - acc: 0.9453 - val_loss: 0.1659 - val_acc: 0.9409\n",
      "Epoch 492/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1307 - acc: 0.9471 - val_loss: 0.1565 - val_acc: 0.9412\n",
      "Epoch 493/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1315 - acc: 0.9463 - val_loss: 0.1751 - val_acc: 0.9353\n",
      "Epoch 494/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1312 - acc: 0.9467 - val_loss: 0.1616 - val_acc: 0.9439\n",
      "Epoch 495/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1302 - acc: 0.9474 - val_loss: 0.1601 - val_acc: 0.9416\n",
      "Epoch 496/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1298 - acc: 0.9472 - val_loss: 0.1388 - val_acc: 0.9492\n",
      "Epoch 497/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27952/27952 [==============================] - 5s - loss: 0.1312 - acc: 0.9476 - val_loss: 0.1590 - val_acc: 0.9440\n",
      "Epoch 498/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1294 - acc: 0.9476 - val_loss: 0.1485 - val_acc: 0.9469\n",
      "Epoch 499/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1310 - acc: 0.9480 - val_loss: 0.1591 - val_acc: 0.9423\n",
      "Epoch 500/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1291 - acc: 0.9472 - val_loss: 0.1474 - val_acc: 0.9463\n",
      "Epoch 501/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1316 - acc: 0.9462 - val_loss: 0.1610 - val_acc: 0.9380\n",
      "Epoch 502/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1324 - acc: 0.9475 - val_loss: 0.1307 - val_acc: 0.9542\n",
      "Epoch 503/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1331 - acc: 0.9458 - val_loss: 0.1576 - val_acc: 0.9422\n",
      "Epoch 504/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1305 - acc: 0.9464 - val_loss: 0.1454 - val_acc: 0.9506\n",
      "Epoch 505/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1294 - acc: 0.9463 - val_loss: 0.1541 - val_acc: 0.9429\n",
      "Epoch 506/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1300 - acc: 0.9467 - val_loss: 0.1488 - val_acc: 0.9455\n",
      "Epoch 507/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1271 - acc: 0.9481 - val_loss: 0.1414 - val_acc: 0.9501\n",
      "Epoch 508/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1289 - acc: 0.9478 - val_loss: 0.1576 - val_acc: 0.9420\n",
      "Epoch 509/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1292 - acc: 0.9492 - val_loss: 0.1475 - val_acc: 0.9482\n",
      "Epoch 510/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1301 - acc: 0.9469 - val_loss: 0.1467 - val_acc: 0.9452\n",
      "Epoch 511/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1315 - acc: 0.9464 - val_loss: 0.1832 - val_acc: 0.9299\n",
      "Epoch 512/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1317 - acc: 0.9478 - val_loss: 0.1428 - val_acc: 0.9472\n",
      "Epoch 513/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1280 - acc: 0.9485 - val_loss: 0.1434 - val_acc: 0.9489\n",
      "Epoch 514/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1302 - acc: 0.9484 - val_loss: 0.1472 - val_acc: 0.9481\n",
      "Epoch 515/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1267 - acc: 0.9491 - val_loss: 0.1639 - val_acc: 0.9438\n",
      "Epoch 516/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1284 - acc: 0.9483 - val_loss: 0.1447 - val_acc: 0.9495\n",
      "Epoch 517/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1272 - acc: 0.9481 - val_loss: 0.1577 - val_acc: 0.9430\n",
      "Epoch 518/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1297 - acc: 0.9477 - val_loss: 0.1455 - val_acc: 0.9466\n",
      "Epoch 519/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1253 - acc: 0.9478 - val_loss: 0.1427 - val_acc: 0.9466\n",
      "Epoch 520/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1289 - acc: 0.9471 - val_loss: 0.1488 - val_acc: 0.9459\n",
      "Epoch 521/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1291 - acc: 0.9473 - val_loss: 0.1581 - val_acc: 0.9388\n",
      "Epoch 522/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1319 - acc: 0.9459 - val_loss: 0.1807 - val_acc: 0.9335\n",
      "Epoch 523/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1272 - acc: 0.9474 - val_loss: 0.1437 - val_acc: 0.9493\n",
      "Epoch 524/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1292 - acc: 0.9474 - val_loss: 0.1430 - val_acc: 0.9506\n",
      "Epoch 525/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1263 - acc: 0.9490 - val_loss: 0.1315 - val_acc: 0.9531\n",
      "Epoch 526/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1311 - acc: 0.9465 - val_loss: 0.1513 - val_acc: 0.9430\n",
      "Epoch 527/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1316 - acc: 0.9477 - val_loss: 0.1471 - val_acc: 0.9450\n",
      "Epoch 528/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1260 - acc: 0.9503 - val_loss: 0.1617 - val_acc: 0.9408\n",
      "Epoch 529/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1279 - acc: 0.9507 - val_loss: 0.1375 - val_acc: 0.9498\n",
      "Epoch 530/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1275 - acc: 0.9479 - val_loss: 0.1548 - val_acc: 0.9442\n",
      "Epoch 531/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1264 - acc: 0.9487 - val_loss: 0.1692 - val_acc: 0.9395\n",
      "Epoch 532/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1284 - acc: 0.9476 - val_loss: 0.1452 - val_acc: 0.9456\n",
      "Epoch 533/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1270 - acc: 0.9487 - val_loss: 0.1790 - val_acc: 0.9375\n",
      "Epoch 534/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1275 - acc: 0.9479 - val_loss: 0.1380 - val_acc: 0.9511\n",
      "Epoch 535/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1276 - acc: 0.9471 - val_loss: 0.1357 - val_acc: 0.9532\n",
      "Epoch 536/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1264 - acc: 0.9484 - val_loss: 0.1463 - val_acc: 0.9459\n",
      "Epoch 537/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1298 - acc: 0.9481 - val_loss: 0.1517 - val_acc: 0.9440\n",
      "Epoch 538/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1281 - acc: 0.9484 - val_loss: 0.1588 - val_acc: 0.9413\n",
      "Epoch 539/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1265 - acc: 0.9488 - val_loss: 0.1445 - val_acc: 0.9503\n",
      "Epoch 540/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1270 - acc: 0.9492 - val_loss: 0.1615 - val_acc: 0.9433\n",
      "Epoch 541/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1253 - acc: 0.9478 - val_loss: 0.1452 - val_acc: 0.9496\n",
      "Epoch 542/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1245 - acc: 0.9496 - val_loss: 0.1501 - val_acc: 0.9430\n",
      "Epoch 543/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1262 - acc: 0.9485 - val_loss: 0.1500 - val_acc: 0.9488\n",
      "Epoch 544/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1265 - acc: 0.9492 - val_loss: 0.1410 - val_acc: 0.9486\n",
      "Epoch 545/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1262 - acc: 0.9500 - val_loss: 0.1766 - val_acc: 0.9359\n",
      "Epoch 546/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1259 - acc: 0.9499 - val_loss: 0.1427 - val_acc: 0.9495\n",
      "Epoch 547/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1272 - acc: 0.9487 - val_loss: 0.1417 - val_acc: 0.9515\n",
      "Epoch 548/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1293 - acc: 0.9469 - val_loss: 0.1451 - val_acc: 0.9465\n",
      "Epoch 549/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1278 - acc: 0.9478 - val_loss: 0.1521 - val_acc: 0.9429\n",
      "Epoch 550/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1255 - acc: 0.9481 - val_loss: 0.1396 - val_acc: 0.9498\n",
      "Epoch 551/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1242 - acc: 0.9495 - val_loss: 0.1739 - val_acc: 0.9419\n",
      "Epoch 552/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1258 - acc: 0.9495 - val_loss: 0.1435 - val_acc: 0.9475\n",
      "Epoch 553/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1236 - acc: 0.9493 - val_loss: 0.1464 - val_acc: 0.9492\n",
      "Epoch 554/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1268 - acc: 0.9476 - val_loss: 0.1524 - val_acc: 0.9440\n",
      "Epoch 555/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1294 - acc: 0.9477 - val_loss: 0.1449 - val_acc: 0.9459\n",
      "Epoch 556/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1239 - acc: 0.9501 - val_loss: 0.1573 - val_acc: 0.9436\n",
      "Epoch 557/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1303 - acc: 0.9483 - val_loss: 0.1448 - val_acc: 0.9488\n",
      "Epoch 558/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1249 - acc: 0.9477 - val_loss: 0.1395 - val_acc: 0.9532\n",
      "Epoch 559/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1285 - acc: 0.9489 - val_loss: 0.1653 - val_acc: 0.9428\n",
      "Epoch 560/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1257 - acc: 0.9492 - val_loss: 0.1452 - val_acc: 0.9471\n",
      "Epoch 561/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1248 - acc: 0.9488 - val_loss: 0.1494 - val_acc: 0.9466\n",
      "Epoch 562/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1259 - acc: 0.9497 - val_loss: 0.1474 - val_acc: 0.9466\n",
      "Epoch 563/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1270 - acc: 0.9487 - val_loss: 0.1573 - val_acc: 0.9446\n",
      "Epoch 564/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1252 - acc: 0.9492 - val_loss: 0.1390 - val_acc: 0.9485\n",
      "Epoch 565/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1262 - acc: 0.9486 - val_loss: 0.1461 - val_acc: 0.9468\n",
      "Epoch 566/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1263 - acc: 0.9477 - val_loss: 0.1478 - val_acc: 0.9439\n",
      "Epoch 567/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1243 - acc: 0.9487 - val_loss: 0.1416 - val_acc: 0.9508\n",
      "Epoch 568/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1281 - acc: 0.9495 - val_loss: 0.1407 - val_acc: 0.9499\n",
      "Epoch 569/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1251 - acc: 0.9487 - val_loss: 0.1379 - val_acc: 0.9506\n",
      "Epoch 570/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1226 - acc: 0.9508 - val_loss: 0.1405 - val_acc: 0.9505\n",
      "Epoch 571/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1246 - acc: 0.9482 - val_loss: 0.1691 - val_acc: 0.9405\n",
      "Epoch 572/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1277 - acc: 0.9500 - val_loss: 0.1535 - val_acc: 0.9436\n",
      "Epoch 573/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1214 - acc: 0.9508 - val_loss: 0.1909 - val_acc: 0.9393\n",
      "Epoch 574/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1265 - acc: 0.9478 - val_loss: 0.1313 - val_acc: 0.9519\n",
      "Epoch 575/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1250 - acc: 0.9488 - val_loss: 0.1553 - val_acc: 0.9450\n",
      "Epoch 576/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1239 - acc: 0.9500 - val_loss: 0.1574 - val_acc: 0.9405\n",
      "Epoch 577/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1203 - acc: 0.9521 - val_loss: 0.1393 - val_acc: 0.9475\n",
      "Epoch 578/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1228 - acc: 0.9488 - val_loss: 0.1571 - val_acc: 0.9419\n",
      "Epoch 579/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1210 - acc: 0.9496 - val_loss: 0.1652 - val_acc: 0.9428\n",
      "Epoch 580/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1220 - acc: 0.9505 - val_loss: 0.1431 - val_acc: 0.9486\n",
      "Epoch 581/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1256 - acc: 0.9489 - val_loss: 0.1743 - val_acc: 0.9366\n",
      "Epoch 582/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1220 - acc: 0.9505 - val_loss: 0.1538 - val_acc: 0.9463\n",
      "Epoch 583/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1237 - acc: 0.9505 - val_loss: 0.1578 - val_acc: 0.9443\n",
      "Epoch 584/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1249 - acc: 0.9500 - val_loss: 0.1620 - val_acc: 0.9400\n",
      "Epoch 585/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1249 - acc: 0.9496 - val_loss: 0.1522 - val_acc: 0.9466\n",
      "Epoch 586/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1243 - acc: 0.9497 - val_loss: 0.1508 - val_acc: 0.9499\n",
      "Epoch 587/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1243 - acc: 0.9490 - val_loss: 0.1363 - val_acc: 0.9513\n",
      "Epoch 588/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1214 - acc: 0.9500 - val_loss: 0.1475 - val_acc: 0.9492\n",
      "Epoch 589/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1256 - acc: 0.9496 - val_loss: 0.1383 - val_acc: 0.9486\n",
      "Epoch 590/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1218 - acc: 0.9502 - val_loss: 0.1683 - val_acc: 0.9405\n",
      "Epoch 591/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1199 - acc: 0.9509 - val_loss: 0.1422 - val_acc: 0.9515\n",
      "Epoch 592/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1213 - acc: 0.9502 - val_loss: 0.1481 - val_acc: 0.9496\n",
      "Epoch 593/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1250 - acc: 0.9489 - val_loss: 0.1591 - val_acc: 0.9420\n",
      "Epoch 594/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1238 - acc: 0.9493 - val_loss: 0.2072 - val_acc: 0.9264\n",
      "Epoch 595/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1232 - acc: 0.9495 - val_loss: 0.1490 - val_acc: 0.9450\n",
      "Epoch 596/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1203 - acc: 0.9518 - val_loss: 0.1476 - val_acc: 0.9466\n",
      "Epoch 597/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1228 - acc: 0.9506 - val_loss: 0.1575 - val_acc: 0.9439\n",
      "Epoch 598/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1239 - acc: 0.9490 - val_loss: 0.1453 - val_acc: 0.9433\n",
      "Epoch 599/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1234 - acc: 0.9498 - val_loss: 0.1792 - val_acc: 0.9292\n",
      "Epoch 600/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1199 - acc: 0.9515 - val_loss: 0.1365 - val_acc: 0.9519\n",
      "Epoch 601/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1246 - acc: 0.9484 - val_loss: 0.1492 - val_acc: 0.9468\n",
      "Epoch 602/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1215 - acc: 0.9511 - val_loss: 0.1316 - val_acc: 0.9552\n",
      "Epoch 603/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1220 - acc: 0.9506 - val_loss: 0.1562 - val_acc: 0.9448\n",
      "Epoch 604/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1244 - acc: 0.9501 - val_loss: 0.1548 - val_acc: 0.9440\n",
      "Epoch 605/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1237 - acc: 0.9506 - val_loss: 0.1436 - val_acc: 0.9502\n",
      "Epoch 606/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1204 - acc: 0.9502 - val_loss: 0.1814 - val_acc: 0.9337\n",
      "Epoch 607/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1202 - acc: 0.9505 - val_loss: 0.1739 - val_acc: 0.9350\n",
      "Epoch 608/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1219 - acc: 0.9505 - val_loss: 0.1651 - val_acc: 0.9419\n",
      "Epoch 609/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1204 - acc: 0.9516 - val_loss: 0.1442 - val_acc: 0.9499\n",
      "Epoch 610/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1210 - acc: 0.9510 - val_loss: 0.1509 - val_acc: 0.9455\n",
      "Epoch 611/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1238 - acc: 0.9491 - val_loss: 0.1451 - val_acc: 0.9463\n",
      "Epoch 612/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1229 - acc: 0.9489 - val_loss: 0.1512 - val_acc: 0.9472\n",
      "Epoch 613/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1211 - acc: 0.9512 - val_loss: 0.1601 - val_acc: 0.9426\n",
      "Epoch 614/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1202 - acc: 0.9521 - val_loss: 0.1661 - val_acc: 0.9425\n",
      "Epoch 615/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1238 - acc: 0.9503 - val_loss: 0.1535 - val_acc: 0.9468\n",
      "Epoch 616/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1235 - acc: 0.9503 - val_loss: 0.1703 - val_acc: 0.9392\n",
      "Epoch 617/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1197 - acc: 0.9526 - val_loss: 0.1290 - val_acc: 0.9536\n",
      "Epoch 618/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1206 - acc: 0.9515 - val_loss: 0.1449 - val_acc: 0.9479\n",
      "Epoch 619/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1213 - acc: 0.9515 - val_loss: 0.1452 - val_acc: 0.9471\n",
      "Epoch 620/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1206 - acc: 0.9509 - val_loss: 0.1666 - val_acc: 0.9385\n",
      "Epoch 621/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27952/27952 [==============================] - 5s - loss: 0.1193 - acc: 0.9510 - val_loss: 0.1500 - val_acc: 0.9469\n",
      "Epoch 622/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1230 - acc: 0.9498 - val_loss: 0.1793 - val_acc: 0.9386\n",
      "Epoch 623/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1217 - acc: 0.9516 - val_loss: 0.1431 - val_acc: 0.9471\n",
      "Epoch 624/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1210 - acc: 0.9507 - val_loss: 0.1676 - val_acc: 0.9415\n",
      "Epoch 625/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1230 - acc: 0.9507 - val_loss: 0.1592 - val_acc: 0.9469\n",
      "Epoch 626/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1219 - acc: 0.9500 - val_loss: 0.1414 - val_acc: 0.9503\n",
      "Epoch 627/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1213 - acc: 0.9512 - val_loss: 0.1394 - val_acc: 0.9526\n",
      "Epoch 628/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1186 - acc: 0.9520 - val_loss: 0.1426 - val_acc: 0.9486\n",
      "Epoch 629/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1218 - acc: 0.9502 - val_loss: 0.1946 - val_acc: 0.9336\n",
      "Epoch 630/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1219 - acc: 0.9507 - val_loss: 0.1745 - val_acc: 0.9399\n",
      "Epoch 631/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1186 - acc: 0.9531 - val_loss: 0.1503 - val_acc: 0.9446\n",
      "Epoch 632/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1213 - acc: 0.9501 - val_loss: 0.1418 - val_acc: 0.9541\n",
      "Epoch 633/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1207 - acc: 0.9505 - val_loss: 0.1497 - val_acc: 0.9462\n",
      "Epoch 634/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1188 - acc: 0.9522 - val_loss: 0.1601 - val_acc: 0.9433\n",
      "Epoch 635/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1196 - acc: 0.9510 - val_loss: 0.1544 - val_acc: 0.9446\n",
      "Epoch 636/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1179 - acc: 0.9525 - val_loss: 0.1398 - val_acc: 0.9481\n",
      "Epoch 637/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1197 - acc: 0.9515 - val_loss: 0.1426 - val_acc: 0.9483\n",
      "Epoch 638/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1214 - acc: 0.9495 - val_loss: 0.1406 - val_acc: 0.9512\n",
      "Epoch 639/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1197 - acc: 0.9509 - val_loss: 0.1590 - val_acc: 0.9462\n",
      "Epoch 640/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.1181 - acc: 0.9507 - val_loss: 0.1403 - val_acc: 0.9509\n",
      "Epoch 641/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1185 - acc: 0.9518 - val_loss: 0.1559 - val_acc: 0.9463\n",
      "Epoch 642/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1212 - acc: 0.9515 - val_loss: 0.2005 - val_acc: 0.9260\n",
      "Epoch 643/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1209 - acc: 0.9515 - val_loss: 0.1654 - val_acc: 0.9435\n",
      "Epoch 644/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1194 - acc: 0.9507 - val_loss: 0.3207 - val_acc: 0.9163\n",
      "Epoch 645/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1221 - acc: 0.9506 - val_loss: 0.1463 - val_acc: 0.9479\n",
      "Epoch 646/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1179 - acc: 0.9518 - val_loss: 0.1456 - val_acc: 0.9471\n",
      "Epoch 647/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1193 - acc: 0.9522 - val_loss: 0.2172 - val_acc: 0.9257\n",
      "Epoch 648/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1164 - acc: 0.9526 - val_loss: 0.1404 - val_acc: 0.9498\n",
      "Epoch 649/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1198 - acc: 0.9512 - val_loss: 0.1640 - val_acc: 0.9403\n",
      "Epoch 650/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1191 - acc: 0.9505 - val_loss: 0.1642 - val_acc: 0.9458\n",
      "Epoch 651/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1168 - acc: 0.9527 - val_loss: 0.1544 - val_acc: 0.9465\n",
      "Epoch 652/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1196 - acc: 0.9518 - val_loss: 0.1465 - val_acc: 0.9462\n",
      "Epoch 653/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1209 - acc: 0.9510 - val_loss: 0.1459 - val_acc: 0.9479\n",
      "Epoch 654/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1185 - acc: 0.9512 - val_loss: 0.1630 - val_acc: 0.9386\n",
      "Epoch 655/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1196 - acc: 0.9517 - val_loss: 0.1401 - val_acc: 0.9539\n",
      "Epoch 656/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1171 - acc: 0.9518 - val_loss: 0.1513 - val_acc: 0.9489\n",
      "Epoch 657/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1216 - acc: 0.9509 - val_loss: 0.1427 - val_acc: 0.9473\n",
      "Epoch 658/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1154 - acc: 0.9530 - val_loss: 0.1381 - val_acc: 0.9508\n",
      "Epoch 659/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1209 - acc: 0.9516 - val_loss: 0.1358 - val_acc: 0.9521\n",
      "Epoch 660/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1161 - acc: 0.9526 - val_loss: 0.1388 - val_acc: 0.9503\n",
      "Epoch 661/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1226 - acc: 0.9501 - val_loss: 0.1276 - val_acc: 0.9546\n",
      "Epoch 662/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1175 - acc: 0.9513 - val_loss: 0.1502 - val_acc: 0.9503\n",
      "Epoch 663/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1186 - acc: 0.9532 - val_loss: 0.1780 - val_acc: 0.9458\n",
      "Epoch 664/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1181 - acc: 0.9504 - val_loss: 0.1487 - val_acc: 0.9503\n",
      "Epoch 665/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1154 - acc: 0.9522 - val_loss: 0.1490 - val_acc: 0.9472\n",
      "Epoch 666/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1194 - acc: 0.9512 - val_loss: 0.1360 - val_acc: 0.9565\n",
      "Epoch 667/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1189 - acc: 0.9521 - val_loss: 0.1470 - val_acc: 0.9496\n",
      "Epoch 668/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1159 - acc: 0.9537 - val_loss: 0.1365 - val_acc: 0.9511\n",
      "Epoch 669/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1196 - acc: 0.9509 - val_loss: 0.1624 - val_acc: 0.9403\n",
      "Epoch 670/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1207 - acc: 0.9510 - val_loss: 0.1422 - val_acc: 0.9503\n",
      "Epoch 671/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1186 - acc: 0.9525 - val_loss: 0.1358 - val_acc: 0.9522\n",
      "Epoch 672/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1167 - acc: 0.9519 - val_loss: 0.1517 - val_acc: 0.9449\n",
      "Epoch 673/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1160 - acc: 0.9525 - val_loss: 0.1329 - val_acc: 0.9511\n",
      "Epoch 674/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1189 - acc: 0.9525 - val_loss: 0.1774 - val_acc: 0.9357\n",
      "Epoch 675/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1164 - acc: 0.9524 - val_loss: 0.1469 - val_acc: 0.9481\n",
      "Epoch 676/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1166 - acc: 0.9524 - val_loss: 0.1443 - val_acc: 0.9521\n",
      "Epoch 677/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1194 - acc: 0.9514 - val_loss: 0.1535 - val_acc: 0.9450\n",
      "Epoch 678/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1175 - acc: 0.9528 - val_loss: 0.1572 - val_acc: 0.9475\n",
      "Epoch 679/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1186 - acc: 0.9518 - val_loss: 0.1520 - val_acc: 0.9491\n",
      "Epoch 680/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1183 - acc: 0.9537 - val_loss: 0.1315 - val_acc: 0.9558\n",
      "Epoch 681/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1156 - acc: 0.9531 - val_loss: 0.1513 - val_acc: 0.9476\n",
      "Epoch 682/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1200 - acc: 0.9516 - val_loss: 0.1477 - val_acc: 0.9486\n",
      "Epoch 683/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1180 - acc: 0.9521 - val_loss: 0.1595 - val_acc: 0.9383\n",
      "Epoch 684/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1199 - acc: 0.9515 - val_loss: 0.1477 - val_acc: 0.9479\n",
      "Epoch 685/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1148 - acc: 0.9535 - val_loss: 0.1262 - val_acc: 0.9549\n",
      "Epoch 686/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1184 - acc: 0.9520 - val_loss: 0.1426 - val_acc: 0.9508\n",
      "Epoch 687/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1168 - acc: 0.9525 - val_loss: 0.1375 - val_acc: 0.9535\n",
      "Epoch 688/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1161 - acc: 0.9522 - val_loss: 0.1579 - val_acc: 0.9425\n",
      "Epoch 689/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1177 - acc: 0.9536 - val_loss: 0.1420 - val_acc: 0.9522\n",
      "Epoch 690/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1162 - acc: 0.9527 - val_loss: 0.1428 - val_acc: 0.9483\n",
      "Epoch 691/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1156 - acc: 0.9528 - val_loss: 0.1378 - val_acc: 0.9533\n",
      "Epoch 692/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1154 - acc: 0.9532 - val_loss: 0.1470 - val_acc: 0.9475\n",
      "Epoch 693/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1159 - acc: 0.9523 - val_loss: 0.1445 - val_acc: 0.9493\n",
      "Epoch 694/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1182 - acc: 0.9521 - val_loss: 0.1447 - val_acc: 0.9472\n",
      "Epoch 695/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1176 - acc: 0.9532 - val_loss: 0.1478 - val_acc: 0.9445\n",
      "Epoch 696/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1148 - acc: 0.9537 - val_loss: 0.1619 - val_acc: 0.9408\n",
      "Epoch 697/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1154 - acc: 0.9534 - val_loss: 0.1608 - val_acc: 0.9435\n",
      "Epoch 698/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1173 - acc: 0.9525 - val_loss: 0.1768 - val_acc: 0.9350\n",
      "Epoch 699/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1178 - acc: 0.9519 - val_loss: 0.1315 - val_acc: 0.9579\n",
      "Epoch 700/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1152 - acc: 0.9524 - val_loss: 0.1428 - val_acc: 0.9512\n",
      "Epoch 701/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1174 - acc: 0.9518 - val_loss: 0.1395 - val_acc: 0.9529\n",
      "Epoch 702/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1156 - acc: 0.9526 - val_loss: 0.1431 - val_acc: 0.9513\n",
      "Epoch 703/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1172 - acc: 0.9534 - val_loss: 0.1425 - val_acc: 0.9509\n",
      "Epoch 704/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1135 - acc: 0.9532 - val_loss: 0.1890 - val_acc: 0.9389\n",
      "Epoch 705/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1178 - acc: 0.9525 - val_loss: 0.1432 - val_acc: 0.9549\n",
      "Epoch 706/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1151 - acc: 0.9532 - val_loss: 0.1409 - val_acc: 0.9501\n",
      "Epoch 707/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1130 - acc: 0.9529 - val_loss: 0.1967 - val_acc: 0.9267\n",
      "Epoch 708/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1164 - acc: 0.9528 - val_loss: 0.1422 - val_acc: 0.9528\n",
      "Epoch 709/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1161 - acc: 0.9526 - val_loss: 0.1345 - val_acc: 0.9519\n",
      "Epoch 710/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1163 - acc: 0.9530 - val_loss: 0.1628 - val_acc: 0.9445\n",
      "Epoch 711/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1153 - acc: 0.9522 - val_loss: 0.1438 - val_acc: 0.9523\n",
      "Epoch 712/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1175 - acc: 0.9518 - val_loss: 0.1452 - val_acc: 0.9496\n",
      "Epoch 713/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1148 - acc: 0.9533 - val_loss: 0.1498 - val_acc: 0.9485\n",
      "Epoch 714/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1143 - acc: 0.9534 - val_loss: 0.1422 - val_acc: 0.9536\n",
      "Epoch 715/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1196 - acc: 0.9514 - val_loss: 0.1524 - val_acc: 0.9436\n",
      "Epoch 716/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1172 - acc: 0.9523 - val_loss: 0.2019 - val_acc: 0.9330\n",
      "Epoch 717/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1163 - acc: 0.9519 - val_loss: 0.1434 - val_acc: 0.9528\n",
      "Epoch 718/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1143 - acc: 0.9539 - val_loss: 0.1358 - val_acc: 0.9529\n",
      "Epoch 719/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1162 - acc: 0.9524 - val_loss: 0.1439 - val_acc: 0.9529\n",
      "Epoch 720/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1144 - acc: 0.9547 - val_loss: 0.1444 - val_acc: 0.9511\n",
      "Epoch 721/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1138 - acc: 0.9544 - val_loss: 0.1379 - val_acc: 0.9545\n",
      "Epoch 722/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1174 - acc: 0.9531 - val_loss: 0.1916 - val_acc: 0.9415\n",
      "Epoch 723/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1168 - acc: 0.9525 - val_loss: 0.1905 - val_acc: 0.9345\n",
      "Epoch 724/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1150 - acc: 0.9526 - val_loss: 0.1437 - val_acc: 0.9485\n",
      "Epoch 725/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1160 - acc: 0.9529 - val_loss: 0.1537 - val_acc: 0.9463\n",
      "Epoch 726/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1152 - acc: 0.9537 - val_loss: 0.1496 - val_acc: 0.9459\n",
      "Epoch 727/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1162 - acc: 0.9539 - val_loss: 0.1536 - val_acc: 0.9450\n",
      "Epoch 728/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1152 - acc: 0.9540 - val_loss: 0.1476 - val_acc: 0.9496\n",
      "Epoch 729/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1139 - acc: 0.9547 - val_loss: 0.1429 - val_acc: 0.9505\n",
      "Epoch 730/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1149 - acc: 0.9520 - val_loss: 0.1457 - val_acc: 0.9513\n",
      "Epoch 731/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1115 - acc: 0.9562 - val_loss: 0.1641 - val_acc: 0.9428\n",
      "Epoch 732/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1147 - acc: 0.9522 - val_loss: 0.1347 - val_acc: 0.9525\n",
      "Epoch 733/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1113 - acc: 0.9553 - val_loss: 0.1415 - val_acc: 0.9518\n",
      "Epoch 734/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1123 - acc: 0.9535 - val_loss: 0.1455 - val_acc: 0.9493\n",
      "Epoch 735/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1140 - acc: 0.9533 - val_loss: 0.1555 - val_acc: 0.9449\n",
      "Epoch 736/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1146 - acc: 0.9544 - val_loss: 0.1391 - val_acc: 0.9496\n",
      "Epoch 737/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1155 - acc: 0.9539 - val_loss: 0.1481 - val_acc: 0.9492\n",
      "Epoch 738/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1126 - acc: 0.9545 - val_loss: 0.1476 - val_acc: 0.9513\n",
      "Epoch 739/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1124 - acc: 0.9532 - val_loss: 0.1630 - val_acc: 0.9452\n",
      "Epoch 740/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1144 - acc: 0.9534 - val_loss: 0.1907 - val_acc: 0.9299\n",
      "Epoch 741/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1140 - acc: 0.9541 - val_loss: 0.1299 - val_acc: 0.9526\n",
      "Epoch 742/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1136 - acc: 0.9538 - val_loss: 0.1456 - val_acc: 0.9513\n",
      "Epoch 743/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1127 - acc: 0.9554 - val_loss: 0.1473 - val_acc: 0.9502\n",
      "Epoch 744/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1137 - acc: 0.9537 - val_loss: 0.1484 - val_acc: 0.9505\n",
      "Epoch 745/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27952/27952 [==============================] - 5s - loss: 0.1143 - acc: 0.9519 - val_loss: 0.1710 - val_acc: 0.9376\n",
      "Epoch 746/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1140 - acc: 0.9519 - val_loss: 0.1819 - val_acc: 0.9347\n",
      "Epoch 747/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1150 - acc: 0.9517 - val_loss: 0.1331 - val_acc: 0.9556\n",
      "Epoch 748/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1144 - acc: 0.9532 - val_loss: 0.1497 - val_acc: 0.9499\n",
      "Epoch 749/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1117 - acc: 0.9548 - val_loss: 0.1385 - val_acc: 0.9518\n",
      "Epoch 750/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1132 - acc: 0.9537 - val_loss: 0.1330 - val_acc: 0.9566\n",
      "Epoch 751/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1109 - acc: 0.9546 - val_loss: 0.1543 - val_acc: 0.9459\n",
      "Epoch 752/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1132 - acc: 0.9550 - val_loss: 0.1434 - val_acc: 0.9503\n",
      "Epoch 753/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1157 - acc: 0.9531 - val_loss: 0.1442 - val_acc: 0.9515\n",
      "Epoch 754/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1119 - acc: 0.9545 - val_loss: 0.1413 - val_acc: 0.9503\n",
      "Epoch 755/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1109 - acc: 0.9542 - val_loss: 0.1384 - val_acc: 0.9523\n",
      "Epoch 756/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1128 - acc: 0.9542 - val_loss: 0.1635 - val_acc: 0.9426\n",
      "Epoch 757/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1136 - acc: 0.9538 - val_loss: 0.1505 - val_acc: 0.9503\n",
      "Epoch 758/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1143 - acc: 0.9525 - val_loss: 0.1876 - val_acc: 0.9342\n",
      "Epoch 759/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1138 - acc: 0.9539 - val_loss: 0.1296 - val_acc: 0.9535\n",
      "Epoch 760/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1095 - acc: 0.9549 - val_loss: 0.1575 - val_acc: 0.9409\n",
      "Epoch 761/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1128 - acc: 0.9531 - val_loss: 0.1648 - val_acc: 0.9501\n",
      "Epoch 762/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1107 - acc: 0.9554 - val_loss: 0.1475 - val_acc: 0.9511\n",
      "Epoch 763/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1103 - acc: 0.9555 - val_loss: 0.1409 - val_acc: 0.9554\n",
      "Epoch 764/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1114 - acc: 0.9549 - val_loss: 0.1488 - val_acc: 0.9506\n",
      "Epoch 765/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1132 - acc: 0.9536 - val_loss: 0.1426 - val_acc: 0.9463\n",
      "Epoch 766/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1118 - acc: 0.9544 - val_loss: 0.1342 - val_acc: 0.9528\n",
      "Epoch 767/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1144 - acc: 0.9539 - val_loss: 0.1501 - val_acc: 0.9479\n",
      "Epoch 768/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1136 - acc: 0.9543 - val_loss: 0.1435 - val_acc: 0.9496\n",
      "Epoch 769/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1121 - acc: 0.9554 - val_loss: 0.1455 - val_acc: 0.9499\n",
      "Epoch 770/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1099 - acc: 0.9542 - val_loss: 0.1435 - val_acc: 0.9546\n",
      "Epoch 771/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1099 - acc: 0.9555 - val_loss: 0.2309 - val_acc: 0.9287\n",
      "Epoch 772/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1155 - acc: 0.9520 - val_loss: 0.1454 - val_acc: 0.9516\n",
      "Epoch 773/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1114 - acc: 0.9541 - val_loss: 0.1337 - val_acc: 0.9549\n",
      "Epoch 774/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1118 - acc: 0.9543 - val_loss: 0.1503 - val_acc: 0.9522\n",
      "Epoch 775/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1133 - acc: 0.9534 - val_loss: 0.1469 - val_acc: 0.9498\n",
      "Epoch 776/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1122 - acc: 0.9532 - val_loss: 0.1470 - val_acc: 0.9479\n",
      "Epoch 777/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1122 - acc: 0.9531 - val_loss: 0.1453 - val_acc: 0.9495\n",
      "Epoch 778/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1125 - acc: 0.9554 - val_loss: 0.1408 - val_acc: 0.9542\n",
      "Epoch 779/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1094 - acc: 0.9546 - val_loss: 0.1378 - val_acc: 0.9535\n",
      "Epoch 780/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1116 - acc: 0.9535 - val_loss: 0.1366 - val_acc: 0.9532\n",
      "Epoch 781/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1111 - acc: 0.9544 - val_loss: 0.1473 - val_acc: 0.9473\n",
      "Epoch 782/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1118 - acc: 0.9537 - val_loss: 0.1509 - val_acc: 0.9502\n",
      "Epoch 783/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1112 - acc: 0.9544 - val_loss: 0.1601 - val_acc: 0.9442\n",
      "Epoch 784/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1104 - acc: 0.9554 - val_loss: 0.1403 - val_acc: 0.9511\n",
      "Epoch 785/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1122 - acc: 0.9553 - val_loss: 0.1461 - val_acc: 0.9526\n",
      "Epoch 786/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1105 - acc: 0.9545 - val_loss: 0.1536 - val_acc: 0.9493\n",
      "Epoch 787/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1090 - acc: 0.9550 - val_loss: 0.1390 - val_acc: 0.9511\n",
      "Epoch 788/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1138 - acc: 0.9546 - val_loss: 0.1502 - val_acc: 0.9492\n",
      "Epoch 789/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1106 - acc: 0.9555 - val_loss: 0.1335 - val_acc: 0.9538\n",
      "Epoch 790/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1131 - acc: 0.9540 - val_loss: 0.1505 - val_acc: 0.9459\n",
      "Epoch 791/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1114 - acc: 0.9543 - val_loss: 0.1594 - val_acc: 0.9452\n",
      "Epoch 792/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1099 - acc: 0.9549 - val_loss: 0.1443 - val_acc: 0.9505\n",
      "Epoch 793/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1099 - acc: 0.9558 - val_loss: 0.1429 - val_acc: 0.9508\n",
      "Epoch 794/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1130 - acc: 0.9536 - val_loss: 0.1959 - val_acc: 0.9292\n",
      "Epoch 795/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1134 - acc: 0.9546 - val_loss: 0.1692 - val_acc: 0.9429\n",
      "Epoch 796/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1123 - acc: 0.9547 - val_loss: 0.1372 - val_acc: 0.9538\n",
      "Epoch 797/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1124 - acc: 0.9538 - val_loss: 0.1679 - val_acc: 0.9453\n",
      "Epoch 798/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1108 - acc: 0.9563 - val_loss: 0.1619 - val_acc: 0.9416\n",
      "Epoch 799/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1108 - acc: 0.9545 - val_loss: 0.1436 - val_acc: 0.9483\n",
      "Epoch 800/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1110 - acc: 0.9531 - val_loss: 0.1566 - val_acc: 0.9406\n",
      "Epoch 801/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1110 - acc: 0.9551 - val_loss: 0.1675 - val_acc: 0.9408\n",
      "Epoch 802/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1097 - acc: 0.9557 - val_loss: 0.1423 - val_acc: 0.9493\n",
      "Epoch 803/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1098 - acc: 0.9551 - val_loss: 0.1583 - val_acc: 0.9423\n",
      "Epoch 804/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1077 - acc: 0.9559 - val_loss: 0.1429 - val_acc: 0.9506\n",
      "Epoch 805/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1108 - acc: 0.9542 - val_loss: 0.1434 - val_acc: 0.9492\n",
      "Epoch 806/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1091 - acc: 0.9562 - val_loss: 0.1275 - val_acc: 0.9566\n",
      "Epoch 807/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1103 - acc: 0.9549 - val_loss: 0.1435 - val_acc: 0.9521\n",
      "Epoch 808/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1109 - acc: 0.9543 - val_loss: 0.1300 - val_acc: 0.9581\n",
      "Epoch 809/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1117 - acc: 0.9552 - val_loss: 0.1525 - val_acc: 0.9453\n",
      "Epoch 810/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1105 - acc: 0.9535 - val_loss: 0.1444 - val_acc: 0.9505\n",
      "Epoch 811/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1116 - acc: 0.9545 - val_loss: 0.1379 - val_acc: 0.9531\n",
      "Epoch 812/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1120 - acc: 0.9549 - val_loss: 0.1713 - val_acc: 0.9386\n",
      "Epoch 813/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1101 - acc: 0.9554 - val_loss: 0.1399 - val_acc: 0.9545\n",
      "Epoch 814/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1088 - acc: 0.9564 - val_loss: 0.1476 - val_acc: 0.9492\n",
      "Epoch 815/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1103 - acc: 0.9542 - val_loss: 0.1518 - val_acc: 0.9472\n",
      "Epoch 816/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1089 - acc: 0.9559 - val_loss: 0.1361 - val_acc: 0.9526\n",
      "Epoch 817/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1111 - acc: 0.9542 - val_loss: 0.1479 - val_acc: 0.9515\n",
      "Epoch 818/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1116 - acc: 0.9544 - val_loss: 0.1454 - val_acc: 0.9495\n",
      "Epoch 819/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1116 - acc: 0.9542 - val_loss: 0.1645 - val_acc: 0.9388\n",
      "Epoch 820/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1098 - acc: 0.9553 - val_loss: 0.1330 - val_acc: 0.9561\n",
      "Epoch 821/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1099 - acc: 0.9542 - val_loss: 0.1310 - val_acc: 0.9562\n",
      "Epoch 822/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1061 - acc: 0.9577 - val_loss: 0.1510 - val_acc: 0.9492\n",
      "Epoch 823/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1123 - acc: 0.9534 - val_loss: 0.1375 - val_acc: 0.9538\n",
      "Epoch 824/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1079 - acc: 0.9561 - val_loss: 0.1455 - val_acc: 0.9513\n",
      "Epoch 825/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1097 - acc: 0.9554 - val_loss: 0.1509 - val_acc: 0.9492\n",
      "Epoch 826/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1083 - acc: 0.9559 - val_loss: 0.1345 - val_acc: 0.9559\n",
      "Epoch 827/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1092 - acc: 0.9554 - val_loss: 0.1368 - val_acc: 0.9512\n",
      "Epoch 828/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1118 - acc: 0.9540 - val_loss: 0.1421 - val_acc: 0.9565\n",
      "Epoch 829/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1084 - acc: 0.9545 - val_loss: 0.1577 - val_acc: 0.9461\n",
      "Epoch 830/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1075 - acc: 0.9566 - val_loss: 0.1663 - val_acc: 0.9423\n",
      "Epoch 831/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1068 - acc: 0.9564 - val_loss: 0.1523 - val_acc: 0.9466\n",
      "Epoch 832/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1100 - acc: 0.9545 - val_loss: 0.1351 - val_acc: 0.9545\n",
      "Epoch 833/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1119 - acc: 0.9549 - val_loss: 0.1275 - val_acc: 0.9559\n",
      "Epoch 834/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1087 - acc: 0.9553 - val_loss: 0.1527 - val_acc: 0.9496\n",
      "Epoch 835/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1092 - acc: 0.9553 - val_loss: 0.1443 - val_acc: 0.9532\n",
      "Epoch 836/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1095 - acc: 0.9553 - val_loss: 0.1397 - val_acc: 0.9506\n",
      "Epoch 837/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.1080 - acc: 0.9562 - val_loss: 0.1358 - val_acc: 0.9526\n",
      "Epoch 838/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1106 - acc: 0.9541 - val_loss: 0.1332 - val_acc: 0.9558\n",
      "Epoch 839/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1098 - acc: 0.9557 - val_loss: 0.1412 - val_acc: 0.9503\n",
      "Epoch 840/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1057 - acc: 0.9556 - val_loss: 0.1539 - val_acc: 0.9486\n",
      "Epoch 841/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1082 - acc: 0.9549 - val_loss: 0.1814 - val_acc: 0.9408\n",
      "Epoch 842/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1084 - acc: 0.9560 - val_loss: 0.1445 - val_acc: 0.9502\n",
      "Epoch 843/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1063 - acc: 0.9557 - val_loss: 0.1431 - val_acc: 0.9498\n",
      "Epoch 844/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1080 - acc: 0.9559 - val_loss: 0.1382 - val_acc: 0.9548\n",
      "Epoch 845/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1061 - acc: 0.9570 - val_loss: 0.1340 - val_acc: 0.9558\n",
      "Epoch 846/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1106 - acc: 0.9555 - val_loss: 0.1330 - val_acc: 0.9571\n",
      "Epoch 847/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1115 - acc: 0.9547 - val_loss: 0.1675 - val_acc: 0.9410\n",
      "Epoch 848/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1071 - acc: 0.9556 - val_loss: 0.1425 - val_acc: 0.9503\n",
      "Epoch 849/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1087 - acc: 0.9557 - val_loss: 0.1429 - val_acc: 0.9536\n",
      "Epoch 850/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1123 - acc: 0.9549 - val_loss: 0.1374 - val_acc: 0.9539\n",
      "Epoch 851/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1073 - acc: 0.9557 - val_loss: 0.1425 - val_acc: 0.9518\n",
      "Epoch 852/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1088 - acc: 0.9550 - val_loss: 0.1455 - val_acc: 0.9538\n",
      "Epoch 853/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1118 - acc: 0.9542 - val_loss: 0.1376 - val_acc: 0.9518\n",
      "Epoch 854/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1073 - acc: 0.9560 - val_loss: 0.1382 - val_acc: 0.9516\n",
      "Epoch 855/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1076 - acc: 0.9549 - val_loss: 0.1514 - val_acc: 0.9516\n",
      "Epoch 856/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1098 - acc: 0.9553 - val_loss: 0.1547 - val_acc: 0.9438\n",
      "Epoch 857/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1084 - acc: 0.9555 - val_loss: 0.1484 - val_acc: 0.9472\n",
      "Epoch 858/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1105 - acc: 0.9563 - val_loss: 0.1433 - val_acc: 0.9512\n",
      "Epoch 859/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1143 - acc: 0.9542 - val_loss: 0.1769 - val_acc: 0.9357\n",
      "Epoch 860/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1087 - acc: 0.9552 - val_loss: 0.1489 - val_acc: 0.9493\n",
      "Epoch 861/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1069 - acc: 0.9552 - val_loss: 0.1320 - val_acc: 0.9555\n",
      "Epoch 862/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1107 - acc: 0.9545 - val_loss: 0.1399 - val_acc: 0.9489\n",
      "Epoch 863/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1078 - acc: 0.9561 - val_loss: 0.1296 - val_acc: 0.9571\n",
      "Epoch 864/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1067 - acc: 0.9558 - val_loss: 0.1858 - val_acc: 0.9367\n",
      "Epoch 865/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1087 - acc: 0.9561 - val_loss: 0.1521 - val_acc: 0.9478\n",
      "Epoch 866/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1106 - acc: 0.9549 - val_loss: 0.1611 - val_acc: 0.9432\n",
      "Epoch 867/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1085 - acc: 0.9555 - val_loss: 0.1355 - val_acc: 0.9594\n",
      "Epoch 868/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1069 - acc: 0.9563 - val_loss: 0.1478 - val_acc: 0.9482\n",
      "Epoch 869/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27952/27952 [==============================] - 5s - loss: 0.1068 - acc: 0.9556 - val_loss: 0.2122 - val_acc: 0.9363\n",
      "Epoch 870/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1071 - acc: 0.9552 - val_loss: 0.1312 - val_acc: 0.9566\n",
      "Epoch 871/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1087 - acc: 0.9557 - val_loss: 0.1510 - val_acc: 0.9476\n",
      "Epoch 872/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1083 - acc: 0.9556 - val_loss: 0.1397 - val_acc: 0.9529\n",
      "Epoch 873/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1097 - acc: 0.9547 - val_loss: 0.1383 - val_acc: 0.9513\n",
      "Epoch 874/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.1090 - acc: 0.9558 - val_loss: 0.1308 - val_acc: 0.9576\n",
      "Epoch 875/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.1080 - acc: 0.9562 - val_loss: 0.1336 - val_acc: 0.9569\n",
      "Epoch 876/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1056 - acc: 0.9565 - val_loss: 0.1237 - val_acc: 0.9598\n",
      "Epoch 877/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1062 - acc: 0.9565 - val_loss: 0.1532 - val_acc: 0.9436\n",
      "Epoch 878/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1052 - acc: 0.9575 - val_loss: 0.1463 - val_acc: 0.9485\n",
      "Epoch 879/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1069 - acc: 0.9559 - val_loss: 0.1339 - val_acc: 0.9542\n",
      "Epoch 880/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1097 - acc: 0.9561 - val_loss: 0.1491 - val_acc: 0.9462\n",
      "Epoch 881/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1048 - acc: 0.9569 - val_loss: 0.1381 - val_acc: 0.9541\n",
      "Epoch 882/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1067 - acc: 0.9566 - val_loss: 0.1402 - val_acc: 0.9548\n",
      "Epoch 883/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1098 - acc: 0.9549 - val_loss: 0.1408 - val_acc: 0.9535\n",
      "Epoch 884/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1069 - acc: 0.9561 - val_loss: 0.1698 - val_acc: 0.9448\n",
      "Epoch 885/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1054 - acc: 0.9578 - val_loss: 0.1385 - val_acc: 0.9528\n",
      "Epoch 886/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1096 - acc: 0.9559 - val_loss: 0.1328 - val_acc: 0.9574\n",
      "Epoch 887/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1064 - acc: 0.9564 - val_loss: 0.1663 - val_acc: 0.9405\n",
      "Epoch 888/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1070 - acc: 0.9560 - val_loss: 0.1404 - val_acc: 0.9515\n",
      "Epoch 889/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1068 - acc: 0.9559 - val_loss: 0.1612 - val_acc: 0.9443\n",
      "Epoch 890/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1079 - acc: 0.9558 - val_loss: 0.1260 - val_acc: 0.9589\n",
      "Epoch 891/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1056 - acc: 0.9554 - val_loss: 0.1286 - val_acc: 0.9561\n",
      "Epoch 892/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1059 - acc: 0.9577 - val_loss: 0.1310 - val_acc: 0.9569\n",
      "Epoch 893/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1074 - acc: 0.9550 - val_loss: 0.1501 - val_acc: 0.9481\n",
      "Epoch 894/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1053 - acc: 0.9563 - val_loss: 0.1363 - val_acc: 0.9525\n",
      "Epoch 895/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1057 - acc: 0.9578 - val_loss: 0.1597 - val_acc: 0.9443\n",
      "Epoch 896/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1052 - acc: 0.9564 - val_loss: 0.1277 - val_acc: 0.9555\n",
      "Epoch 897/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1058 - acc: 0.9556 - val_loss: 0.1370 - val_acc: 0.9531\n",
      "Epoch 898/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1052 - acc: 0.9568 - val_loss: 0.1432 - val_acc: 0.9512\n",
      "Epoch 899/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1039 - acc: 0.9581 - val_loss: 0.1418 - val_acc: 0.9528\n",
      "Epoch 900/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1030 - acc: 0.9571 - val_loss: 0.1328 - val_acc: 0.9575\n",
      "Epoch 901/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1044 - acc: 0.9580 - val_loss: 0.1356 - val_acc: 0.9531\n",
      "Epoch 902/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1096 - acc: 0.9551 - val_loss: 0.1680 - val_acc: 0.9426\n",
      "Epoch 903/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1048 - acc: 0.9578 - val_loss: 0.1330 - val_acc: 0.9532\n",
      "Epoch 904/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1043 - acc: 0.9569 - val_loss: 0.1420 - val_acc: 0.9505\n",
      "Epoch 905/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1060 - acc: 0.9562 - val_loss: 0.1332 - val_acc: 0.9564\n",
      "Epoch 906/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1055 - acc: 0.9555 - val_loss: 0.1268 - val_acc: 0.9552\n",
      "Epoch 907/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1085 - acc: 0.9562 - val_loss: 0.1448 - val_acc: 0.9503\n",
      "Epoch 908/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1071 - acc: 0.9567 - val_loss: 0.1511 - val_acc: 0.9485\n",
      "Epoch 909/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1067 - acc: 0.9568 - val_loss: 0.1435 - val_acc: 0.9515\n",
      "Epoch 910/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1050 - acc: 0.9575 - val_loss: 0.1344 - val_acc: 0.9533\n",
      "Epoch 911/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1059 - acc: 0.9570 - val_loss: 0.1415 - val_acc: 0.9519\n",
      "Epoch 912/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1064 - acc: 0.9571 - val_loss: 0.1461 - val_acc: 0.9498\n",
      "Epoch 913/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1055 - acc: 0.9546 - val_loss: 0.1519 - val_acc: 0.9446\n",
      "Epoch 914/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1074 - acc: 0.9571 - val_loss: 0.1296 - val_acc: 0.9601\n",
      "Epoch 915/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.1019 - acc: 0.9581 - val_loss: 0.1435 - val_acc: 0.9523\n",
      "Epoch 916/1000\n",
      "27952/27952 [==============================] - 9s - loss: 0.1061 - acc: 0.9566 - val_loss: 0.1436 - val_acc: 0.9481\n",
      "Epoch 917/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1048 - acc: 0.9564 - val_loss: 0.1495 - val_acc: 0.9469\n",
      "Epoch 918/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1025 - acc: 0.9577 - val_loss: 0.1392 - val_acc: 0.9533\n",
      "Epoch 919/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1069 - acc: 0.9557 - val_loss: 0.1376 - val_acc: 0.9526\n",
      "Epoch 920/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1033 - acc: 0.9575 - val_loss: 0.1332 - val_acc: 0.9561\n",
      "Epoch 921/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1065 - acc: 0.9572 - val_loss: 0.1418 - val_acc: 0.9483\n",
      "Epoch 922/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1057 - acc: 0.9570 - val_loss: 0.1400 - val_acc: 0.9531\n",
      "Epoch 923/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1078 - acc: 0.9558 - val_loss: 0.1352 - val_acc: 0.9554\n",
      "Epoch 924/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1046 - acc: 0.9565 - val_loss: 0.1342 - val_acc: 0.9538\n",
      "Epoch 925/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1092 - acc: 0.9566 - val_loss: 0.1410 - val_acc: 0.9549\n",
      "Epoch 926/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1063 - acc: 0.9571 - val_loss: 0.1438 - val_acc: 0.9516\n",
      "Epoch 927/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1024 - acc: 0.9583 - val_loss: 0.1446 - val_acc: 0.9538\n",
      "Epoch 928/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1044 - acc: 0.9570 - val_loss: 0.1466 - val_acc: 0.9469\n",
      "Epoch 929/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1063 - acc: 0.9560 - val_loss: 0.1350 - val_acc: 0.9544\n",
      "Epoch 930/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1065 - acc: 0.9567 - val_loss: 0.1290 - val_acc: 0.9578\n",
      "Epoch 931/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1039 - acc: 0.9563 - val_loss: 0.1956 - val_acc: 0.9224\n",
      "Epoch 932/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1063 - acc: 0.9563 - val_loss: 0.1411 - val_acc: 0.9519\n",
      "Epoch 933/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1023 - acc: 0.9587 - val_loss: 0.1380 - val_acc: 0.9532\n",
      "Epoch 934/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1036 - acc: 0.9582 - val_loss: 0.1355 - val_acc: 0.9535\n",
      "Epoch 935/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1054 - acc: 0.9564 - val_loss: 0.1624 - val_acc: 0.9410\n",
      "Epoch 936/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1053 - acc: 0.9583 - val_loss: 0.1451 - val_acc: 0.9476\n",
      "Epoch 937/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1037 - acc: 0.9573 - val_loss: 0.1368 - val_acc: 0.9525\n",
      "Epoch 938/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1049 - acc: 0.9566 - val_loss: 0.1377 - val_acc: 0.9522\n",
      "Epoch 939/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1062 - acc: 0.9565 - val_loss: 0.1375 - val_acc: 0.9554\n",
      "Epoch 940/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1040 - acc: 0.9563 - val_loss: 0.1414 - val_acc: 0.9554\n",
      "Epoch 941/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1055 - acc: 0.9557 - val_loss: 0.1392 - val_acc: 0.9519\n",
      "Epoch 942/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1046 - acc: 0.9572 - val_loss: 0.1313 - val_acc: 0.9539\n",
      "Epoch 943/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1047 - acc: 0.9565 - val_loss: 0.1383 - val_acc: 0.9501\n",
      "Epoch 944/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1019 - acc: 0.9580 - val_loss: 0.1390 - val_acc: 0.9501\n",
      "Epoch 945/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1028 - acc: 0.9585 - val_loss: 0.1348 - val_acc: 0.9544\n",
      "Epoch 946/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1059 - acc: 0.9575 - val_loss: 0.1460 - val_acc: 0.9503\n",
      "Epoch 947/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1033 - acc: 0.9580 - val_loss: 0.1335 - val_acc: 0.9538\n",
      "Epoch 948/1000\n",
      "27952/27952 [==============================] - 10s - loss: 0.1066 - acc: 0.9561 - val_loss: 0.1492 - val_acc: 0.9513\n",
      "Epoch 949/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1042 - acc: 0.9574 - val_loss: 0.1279 - val_acc: 0.9582\n",
      "Epoch 950/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1038 - acc: 0.9572 - val_loss: 0.1519 - val_acc: 0.9493\n",
      "Epoch 951/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1054 - acc: 0.9572 - val_loss: 0.1433 - val_acc: 0.9509\n",
      "Epoch 952/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1035 - acc: 0.9565 - val_loss: 0.1369 - val_acc: 0.9539\n",
      "Epoch 953/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1078 - acc: 0.9557 - val_loss: 0.1363 - val_acc: 0.9531\n",
      "Epoch 954/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1045 - acc: 0.9567 - val_loss: 0.1325 - val_acc: 0.9551\n",
      "Epoch 955/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1061 - acc: 0.9571 - val_loss: 0.1461 - val_acc: 0.9518\n",
      "Epoch 956/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1027 - acc: 0.9582 - val_loss: 0.1427 - val_acc: 0.9548\n",
      "Epoch 957/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1047 - acc: 0.9567 - val_loss: 0.1334 - val_acc: 0.9564\n",
      "Epoch 958/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1037 - acc: 0.9581 - val_loss: 0.1472 - val_acc: 0.9463\n",
      "Epoch 959/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1033 - acc: 0.9572 - val_loss: 0.1377 - val_acc: 0.9516\n",
      "Epoch 960/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1050 - acc: 0.9575 - val_loss: 0.1335 - val_acc: 0.9565\n",
      "Epoch 961/1000\n",
      "27952/27952 [==============================] - 8s - loss: 0.1054 - acc: 0.9576 - val_loss: 0.1309 - val_acc: 0.9554\n",
      "Epoch 962/1000\n",
      "27952/27952 [==============================] - 7s - loss: 0.1021 - acc: 0.9584 - val_loss: 0.1321 - val_acc: 0.9556\n",
      "Epoch 963/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1014 - acc: 0.9582 - val_loss: 0.1585 - val_acc: 0.9438\n",
      "Epoch 964/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1073 - acc: 0.9575 - val_loss: 0.1319 - val_acc: 0.9564\n",
      "Epoch 965/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1051 - acc: 0.9573 - val_loss: 0.1447 - val_acc: 0.9519\n",
      "Epoch 966/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1045 - acc: 0.9567 - val_loss: 0.1395 - val_acc: 0.9531\n",
      "Epoch 967/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1032 - acc: 0.9574 - val_loss: 0.1416 - val_acc: 0.9533\n",
      "Epoch 968/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1038 - acc: 0.9575 - val_loss: 0.1473 - val_acc: 0.9518\n",
      "Epoch 969/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1036 - acc: 0.9574 - val_loss: 0.1348 - val_acc: 0.9544\n",
      "Epoch 970/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1036 - acc: 0.9577 - val_loss: 0.1294 - val_acc: 0.9559\n",
      "Epoch 971/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1064 - acc: 0.9561 - val_loss: 0.1375 - val_acc: 0.9558\n",
      "Epoch 972/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1029 - acc: 0.9582 - val_loss: 0.1387 - val_acc: 0.9549\n",
      "Epoch 973/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1052 - acc: 0.9572 - val_loss: 0.2201 - val_acc: 0.9286\n",
      "Epoch 974/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1028 - acc: 0.9573 - val_loss: 0.1268 - val_acc: 0.9585\n",
      "Epoch 975/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1037 - acc: 0.9571 - val_loss: 0.1471 - val_acc: 0.9513\n",
      "Epoch 976/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1047 - acc: 0.9579 - val_loss: 0.1477 - val_acc: 0.9488\n",
      "Epoch 977/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1041 - acc: 0.9569 - val_loss: 0.1261 - val_acc: 0.9594\n",
      "Epoch 978/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1013 - acc: 0.9593 - val_loss: 0.1442 - val_acc: 0.9503\n",
      "Epoch 979/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1012 - acc: 0.9590 - val_loss: 0.1528 - val_acc: 0.9433\n",
      "Epoch 980/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1078 - acc: 0.9556 - val_loss: 0.1333 - val_acc: 0.9561\n",
      "Epoch 981/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1026 - acc: 0.9574 - val_loss: 0.1316 - val_acc: 0.9531\n",
      "Epoch 982/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1027 - acc: 0.9592 - val_loss: 0.1408 - val_acc: 0.9518\n",
      "Epoch 983/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1015 - acc: 0.9579 - val_loss: 0.1476 - val_acc: 0.9493\n",
      "Epoch 984/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1016 - acc: 0.9581 - val_loss: 0.1579 - val_acc: 0.9448\n",
      "Epoch 985/1000\n",
      "27952/27952 [==============================] - 6s - loss: 0.1022 - acc: 0.9593 - val_loss: 0.1371 - val_acc: 0.9566\n",
      "Epoch 986/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1038 - acc: 0.9577 - val_loss: 0.1319 - val_acc: 0.9572\n",
      "Epoch 987/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1024 - acc: 0.9582 - val_loss: 0.1403 - val_acc: 0.9559\n",
      "Epoch 988/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1030 - acc: 0.9580 - val_loss: 0.1333 - val_acc: 0.9574\n",
      "Epoch 989/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1030 - acc: 0.9567 - val_loss: 0.1308 - val_acc: 0.9591\n",
      "Epoch 990/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1017 - acc: 0.9582 - val_loss: 0.1371 - val_acc: 0.9581\n",
      "Epoch 991/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1028 - acc: 0.9571 - val_loss: 0.1454 - val_acc: 0.9528\n",
      "Epoch 992/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1057 - acc: 0.9586 - val_loss: 0.1365 - val_acc: 0.9571\n",
      "Epoch 993/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27952/27952 [==============================] - 5s - loss: 0.1023 - acc: 0.9587 - val_loss: 0.1382 - val_acc: 0.9489\n",
      "Epoch 994/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1019 - acc: 0.9586 - val_loss: 0.1339 - val_acc: 0.9554\n",
      "Epoch 995/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1054 - acc: 0.9560 - val_loss: 0.1357 - val_acc: 0.9568\n",
      "Epoch 996/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1000 - acc: 0.9585 - val_loss: 0.1366 - val_acc: 0.9544\n",
      "Epoch 997/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1018 - acc: 0.9575 - val_loss: 0.1734 - val_acc: 0.9443\n",
      "Epoch 998/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1011 - acc: 0.9584 - val_loss: 0.1435 - val_acc: 0.9516\n",
      "Epoch 999/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1009 - acc: 0.9593 - val_loss: 0.1523 - val_acc: 0.9476\n",
      "Epoch 1000/1000\n",
      "27952/27952 [==============================] - 5s - loss: 0.1015 - acc: 0.9590 - val_loss: 0.1662 - val_acc: 0.9385\n"
     ]
    }
   ],
   "source": [
    "history=classifier.fit(X, y,validation_split=0.2,batch_size = 10, nb_epoch = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acc', 'loss', 'val_acc', 'val_loss']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXetrY9lLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fec5c287b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fec5c2b7d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = classifier.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "classifier.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.937"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_target,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "galaxy=pd.read_csv(\"cat_gal_04.csv\")\n",
    "star=pd.read_csv(\"cat_star_04.csv\")\n",
    "\n",
    "basename='flux_nb'\n",
    "initialnumber=455;\n",
    "requiredColumns=[]\n",
    "for i in range(40):\n",
    "    requiredColumns.append(basename+str(initialnumber+10*i))\n",
    "\n",
    "\n",
    "# Copying the required Data alone to a new data frame\n",
    "starTraining=pd.DataFrame()\n",
    "galaxyTraining=pd.DataFrame()\n",
    "for i in requiredColumns:\n",
    "    starTraining[i]=star[i]\n",
    "    galaxyTraining[i]=galaxy[i]\n",
    "\n",
    "\n",
    "starTraining['class']=0\n",
    "galaxyTraining['class']=1\n",
    "\n",
    "testing=pd.concat([starTraining,galaxyTraining])\n",
    "test_target=testing['class']\n",
    "\n",
    "del testing['class']\n",
    "\n",
    "X=testing.values\n",
    "Y=test_target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flux_nb455</th>\n",
       "      <th>flux_nb465</th>\n",
       "      <th>flux_nb475</th>\n",
       "      <th>flux_nb485</th>\n",
       "      <th>flux_nb495</th>\n",
       "      <th>flux_nb505</th>\n",
       "      <th>flux_nb515</th>\n",
       "      <th>flux_nb525</th>\n",
       "      <th>flux_nb535</th>\n",
       "      <th>flux_nb545</th>\n",
       "      <th>...</th>\n",
       "      <th>flux_nb765</th>\n",
       "      <th>flux_nb775</th>\n",
       "      <th>flux_nb785</th>\n",
       "      <th>flux_nb795</th>\n",
       "      <th>flux_nb805</th>\n",
       "      <th>flux_nb815</th>\n",
       "      <th>flux_nb825</th>\n",
       "      <th>flux_nb835</th>\n",
       "      <th>flux_nb845</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.92986</td>\n",
       "      <td>-3.32708</td>\n",
       "      <td>0.164769</td>\n",
       "      <td>6.26762</td>\n",
       "      <td>8.14376</td>\n",
       "      <td>2.86916</td>\n",
       "      <td>6.63043</td>\n",
       "      <td>6.10373</td>\n",
       "      <td>6.27706</td>\n",
       "      <td>5.53524</td>\n",
       "      <td>...</td>\n",
       "      <td>21.6058</td>\n",
       "      <td>19.4299</td>\n",
       "      <td>26.7860</td>\n",
       "      <td>21.5572</td>\n",
       "      <td>14.8760</td>\n",
       "      <td>22.3911</td>\n",
       "      <td>25.3999</td>\n",
       "      <td>24.3821</td>\n",
       "      <td>25.0108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.52590</td>\n",
       "      <td>28.05860</td>\n",
       "      <td>24.309400</td>\n",
       "      <td>25.30680</td>\n",
       "      <td>26.02040</td>\n",
       "      <td>27.41660</td>\n",
       "      <td>28.58850</td>\n",
       "      <td>28.42400</td>\n",
       "      <td>26.79760</td>\n",
       "      <td>35.83690</td>\n",
       "      <td>...</td>\n",
       "      <td>30.5420</td>\n",
       "      <td>33.6569</td>\n",
       "      <td>33.5219</td>\n",
       "      <td>32.0281</td>\n",
       "      <td>39.6865</td>\n",
       "      <td>35.2799</td>\n",
       "      <td>29.7202</td>\n",
       "      <td>37.5383</td>\n",
       "      <td>37.5809</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.32470</td>\n",
       "      <td>11.69490</td>\n",
       "      <td>14.104500</td>\n",
       "      <td>11.29830</td>\n",
       "      <td>9.14734</td>\n",
       "      <td>8.11625</td>\n",
       "      <td>9.37619</td>\n",
       "      <td>14.95640</td>\n",
       "      <td>16.38960</td>\n",
       "      <td>15.68470</td>\n",
       "      <td>...</td>\n",
       "      <td>71.6377</td>\n",
       "      <td>60.8721</td>\n",
       "      <td>75.7418</td>\n",
       "      <td>70.2047</td>\n",
       "      <td>80.8307</td>\n",
       "      <td>81.7389</td>\n",
       "      <td>85.5125</td>\n",
       "      <td>94.8986</td>\n",
       "      <td>85.1857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.50019</td>\n",
       "      <td>14.03800</td>\n",
       "      <td>14.837800</td>\n",
       "      <td>11.21410</td>\n",
       "      <td>10.64130</td>\n",
       "      <td>11.56490</td>\n",
       "      <td>11.59360</td>\n",
       "      <td>14.57450</td>\n",
       "      <td>21.89650</td>\n",
       "      <td>18.88010</td>\n",
       "      <td>...</td>\n",
       "      <td>123.6430</td>\n",
       "      <td>125.9740</td>\n",
       "      <td>139.2400</td>\n",
       "      <td>153.2650</td>\n",
       "      <td>182.9510</td>\n",
       "      <td>184.4680</td>\n",
       "      <td>194.0290</td>\n",
       "      <td>184.9250</td>\n",
       "      <td>178.1220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.62390</td>\n",
       "      <td>13.76380</td>\n",
       "      <td>23.007000</td>\n",
       "      <td>24.28930</td>\n",
       "      <td>26.28510</td>\n",
       "      <td>17.68540</td>\n",
       "      <td>20.63140</td>\n",
       "      <td>21.38260</td>\n",
       "      <td>22.84940</td>\n",
       "      <td>30.51300</td>\n",
       "      <td>...</td>\n",
       "      <td>32.3622</td>\n",
       "      <td>32.5666</td>\n",
       "      <td>27.6383</td>\n",
       "      <td>24.4480</td>\n",
       "      <td>37.8403</td>\n",
       "      <td>28.3031</td>\n",
       "      <td>37.9221</td>\n",
       "      <td>32.1755</td>\n",
       "      <td>29.6770</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   flux_nb455  flux_nb465  flux_nb475  flux_nb485  flux_nb495  flux_nb505  \\\n",
       "0     1.92986    -3.32708    0.164769     6.26762     8.14376     2.86916   \n",
       "1    22.52590    28.05860   24.309400    25.30680    26.02040    27.41660   \n",
       "2    15.32470    11.69490   14.104500    11.29830     9.14734     8.11625   \n",
       "3     5.50019    14.03800   14.837800    11.21410    10.64130    11.56490   \n",
       "4    19.62390    13.76380   23.007000    24.28930    26.28510    17.68540   \n",
       "\n",
       "   flux_nb515  flux_nb525  flux_nb535  flux_nb545  ...    flux_nb765  \\\n",
       "0     6.63043     6.10373     6.27706     5.53524  ...       21.6058   \n",
       "1    28.58850    28.42400    26.79760    35.83690  ...       30.5420   \n",
       "2     9.37619    14.95640    16.38960    15.68470  ...       71.6377   \n",
       "3    11.59360    14.57450    21.89650    18.88010  ...      123.6430   \n",
       "4    20.63140    21.38260    22.84940    30.51300  ...       32.3622   \n",
       "\n",
       "   flux_nb775  flux_nb785  flux_nb795  flux_nb805  flux_nb815  flux_nb825  \\\n",
       "0     19.4299     26.7860     21.5572     14.8760     22.3911     25.3999   \n",
       "1     33.6569     33.5219     32.0281     39.6865     35.2799     29.7202   \n",
       "2     60.8721     75.7418     70.2047     80.8307     81.7389     85.5125   \n",
       "3    125.9740    139.2400    153.2650    182.9510    184.4680    194.0290   \n",
       "4     32.5666     27.6383     24.4480     37.8403     28.3031     37.9221   \n",
       "\n",
       "   flux_nb835  flux_nb845  class  \n",
       "0     24.3821     25.0108      0  \n",
       "1     37.5383     37.5809      0  \n",
       "2     94.8986     85.1857      0  \n",
       "3    184.9250    178.1220      0  \n",
       "4     32.1755     29.6770      0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starTraining.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flux_nb455</th>\n",
       "      <th>flux_nb465</th>\n",
       "      <th>flux_nb475</th>\n",
       "      <th>flux_nb485</th>\n",
       "      <th>flux_nb495</th>\n",
       "      <th>flux_nb505</th>\n",
       "      <th>flux_nb515</th>\n",
       "      <th>flux_nb525</th>\n",
       "      <th>flux_nb535</th>\n",
       "      <th>flux_nb545</th>\n",
       "      <th>...</th>\n",
       "      <th>flux_nb765</th>\n",
       "      <th>flux_nb775</th>\n",
       "      <th>flux_nb785</th>\n",
       "      <th>flux_nb795</th>\n",
       "      <th>flux_nb805</th>\n",
       "      <th>flux_nb815</th>\n",
       "      <th>flux_nb825</th>\n",
       "      <th>flux_nb835</th>\n",
       "      <th>flux_nb845</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.44900</td>\n",
       "      <td>132.61500</td>\n",
       "      <td>158.91300</td>\n",
       "      <td>145.590000</td>\n",
       "      <td>163.37600</td>\n",
       "      <td>183.78900</td>\n",
       "      <td>179.6710</td>\n",
       "      <td>192.89400</td>\n",
       "      <td>194.82400</td>\n",
       "      <td>207.50500</td>\n",
       "      <td>...</td>\n",
       "      <td>342.8200</td>\n",
       "      <td>357.3710</td>\n",
       "      <td>353.5520</td>\n",
       "      <td>457.019000</td>\n",
       "      <td>560.34300</td>\n",
       "      <td>416.3070</td>\n",
       "      <td>425.26600</td>\n",
       "      <td>381.0140</td>\n",
       "      <td>386.5730</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.49438</td>\n",
       "      <td>-15.83780</td>\n",
       "      <td>30.58920</td>\n",
       "      <td>0.362942</td>\n",
       "      <td>21.03530</td>\n",
       "      <td>8.46712</td>\n",
       "      <td>17.5348</td>\n",
       "      <td>-3.50487</td>\n",
       "      <td>14.90490</td>\n",
       "      <td>13.84250</td>\n",
       "      <td>...</td>\n",
       "      <td>28.5056</td>\n",
       "      <td>30.2411</td>\n",
       "      <td>24.4393</td>\n",
       "      <td>34.687200</td>\n",
       "      <td>67.12250</td>\n",
       "      <td>52.6790</td>\n",
       "      <td>42.43470</td>\n",
       "      <td>65.0121</td>\n",
       "      <td>54.9258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.99040</td>\n",
       "      <td>6.52502</td>\n",
       "      <td>6.43737</td>\n",
       "      <td>12.775300</td>\n",
       "      <td>10.38880</td>\n",
       "      <td>12.75550</td>\n",
       "      <td>13.0950</td>\n",
       "      <td>7.92642</td>\n",
       "      <td>20.85460</td>\n",
       "      <td>19.00850</td>\n",
       "      <td>...</td>\n",
       "      <td>26.7839</td>\n",
       "      <td>25.0494</td>\n",
       "      <td>25.8454</td>\n",
       "      <td>23.408600</td>\n",
       "      <td>20.86300</td>\n",
       "      <td>21.7851</td>\n",
       "      <td>18.50620</td>\n",
       "      <td>16.8976</td>\n",
       "      <td>31.7104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.65172</td>\n",
       "      <td>3.94335</td>\n",
       "      <td>14.68870</td>\n",
       "      <td>1.341380</td>\n",
       "      <td>14.88580</td>\n",
       "      <td>5.39008</td>\n",
       "      <td>20.8863</td>\n",
       "      <td>14.02910</td>\n",
       "      <td>10.22620</td>\n",
       "      <td>12.80650</td>\n",
       "      <td>...</td>\n",
       "      <td>46.3418</td>\n",
       "      <td>28.3742</td>\n",
       "      <td>44.3127</td>\n",
       "      <td>24.091500</td>\n",
       "      <td>30.87720</td>\n",
       "      <td>37.5018</td>\n",
       "      <td>39.57710</td>\n",
       "      <td>51.3769</td>\n",
       "      <td>31.6978</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.07877</td>\n",
       "      <td>2.90570</td>\n",
       "      <td>4.61733</td>\n",
       "      <td>8.326370</td>\n",
       "      <td>-4.03998</td>\n",
       "      <td>5.76601</td>\n",
       "      <td>14.9373</td>\n",
       "      <td>13.72290</td>\n",
       "      <td>9.31302</td>\n",
       "      <td>7.58613</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0270</td>\n",
       "      <td>17.2385</td>\n",
       "      <td>1.1008</td>\n",
       "      <td>-0.443576</td>\n",
       "      <td>9.05843</td>\n",
       "      <td>19.8146</td>\n",
       "      <td>8.06216</td>\n",
       "      <td>21.1591</td>\n",
       "      <td>14.1416</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   flux_nb455  flux_nb465  flux_nb475  flux_nb485  flux_nb495  flux_nb505  \\\n",
       "0   123.44900   132.61500   158.91300  145.590000   163.37600   183.78900   \n",
       "1     8.49438   -15.83780    30.58920    0.362942    21.03530     8.46712   \n",
       "2    12.99040     6.52502     6.43737   12.775300    10.38880    12.75550   \n",
       "3     9.65172     3.94335    14.68870    1.341380    14.88580     5.39008   \n",
       "4     1.07877     2.90570     4.61733    8.326370    -4.03998     5.76601   \n",
       "\n",
       "   flux_nb515  flux_nb525  flux_nb535  flux_nb545  ...    flux_nb765  \\\n",
       "0    179.6710   192.89400   194.82400   207.50500  ...      342.8200   \n",
       "1     17.5348    -3.50487    14.90490    13.84250  ...       28.5056   \n",
       "2     13.0950     7.92642    20.85460    19.00850  ...       26.7839   \n",
       "3     20.8863    14.02910    10.22620    12.80650  ...       46.3418   \n",
       "4     14.9373    13.72290     9.31302     7.58613  ...       14.0270   \n",
       "\n",
       "   flux_nb775  flux_nb785  flux_nb795  flux_nb805  flux_nb815  flux_nb825  \\\n",
       "0    357.3710    353.5520  457.019000   560.34300    416.3070   425.26600   \n",
       "1     30.2411     24.4393   34.687200    67.12250     52.6790    42.43470   \n",
       "2     25.0494     25.8454   23.408600    20.86300     21.7851    18.50620   \n",
       "3     28.3742     44.3127   24.091500    30.87720     37.5018    39.57710   \n",
       "4     17.2385      1.1008   -0.443576     9.05843     19.8146     8.06216   \n",
       "\n",
       "   flux_nb835  flux_nb845  class  \n",
       "0    381.0140    386.5730      1  \n",
       "1     65.0121     54.9258      1  \n",
       "2     16.8976     31.7104      1  \n",
       "3     51.3769     31.6978      1  \n",
       "4     21.1591     14.1416      1  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "galaxyTraining.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flux_nb455</th>\n",
       "      <th>flux_nb465</th>\n",
       "      <th>flux_nb475</th>\n",
       "      <th>flux_nb485</th>\n",
       "      <th>flux_nb495</th>\n",
       "      <th>flux_nb505</th>\n",
       "      <th>flux_nb515</th>\n",
       "      <th>flux_nb525</th>\n",
       "      <th>flux_nb535</th>\n",
       "      <th>flux_nb545</th>\n",
       "      <th>...</th>\n",
       "      <th>flux_nb765</th>\n",
       "      <th>flux_nb775</th>\n",
       "      <th>flux_nb785</th>\n",
       "      <th>flux_nb795</th>\n",
       "      <th>flux_nb805</th>\n",
       "      <th>flux_nb815</th>\n",
       "      <th>flux_nb825</th>\n",
       "      <th>flux_nb835</th>\n",
       "      <th>flux_nb845</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.92986</td>\n",
       "      <td>-3.32708</td>\n",
       "      <td>0.164769</td>\n",
       "      <td>6.26762</td>\n",
       "      <td>8.14376</td>\n",
       "      <td>2.86916</td>\n",
       "      <td>6.63043</td>\n",
       "      <td>6.10373</td>\n",
       "      <td>6.27706</td>\n",
       "      <td>5.53524</td>\n",
       "      <td>...</td>\n",
       "      <td>21.6058</td>\n",
       "      <td>19.4299</td>\n",
       "      <td>26.7860</td>\n",
       "      <td>21.5572</td>\n",
       "      <td>14.8760</td>\n",
       "      <td>22.3911</td>\n",
       "      <td>25.3999</td>\n",
       "      <td>24.3821</td>\n",
       "      <td>25.0108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.52590</td>\n",
       "      <td>28.05860</td>\n",
       "      <td>24.309400</td>\n",
       "      <td>25.30680</td>\n",
       "      <td>26.02040</td>\n",
       "      <td>27.41660</td>\n",
       "      <td>28.58850</td>\n",
       "      <td>28.42400</td>\n",
       "      <td>26.79760</td>\n",
       "      <td>35.83690</td>\n",
       "      <td>...</td>\n",
       "      <td>30.5420</td>\n",
       "      <td>33.6569</td>\n",
       "      <td>33.5219</td>\n",
       "      <td>32.0281</td>\n",
       "      <td>39.6865</td>\n",
       "      <td>35.2799</td>\n",
       "      <td>29.7202</td>\n",
       "      <td>37.5383</td>\n",
       "      <td>37.5809</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.32470</td>\n",
       "      <td>11.69490</td>\n",
       "      <td>14.104500</td>\n",
       "      <td>11.29830</td>\n",
       "      <td>9.14734</td>\n",
       "      <td>8.11625</td>\n",
       "      <td>9.37619</td>\n",
       "      <td>14.95640</td>\n",
       "      <td>16.38960</td>\n",
       "      <td>15.68470</td>\n",
       "      <td>...</td>\n",
       "      <td>71.6377</td>\n",
       "      <td>60.8721</td>\n",
       "      <td>75.7418</td>\n",
       "      <td>70.2047</td>\n",
       "      <td>80.8307</td>\n",
       "      <td>81.7389</td>\n",
       "      <td>85.5125</td>\n",
       "      <td>94.8986</td>\n",
       "      <td>85.1857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.50019</td>\n",
       "      <td>14.03800</td>\n",
       "      <td>14.837800</td>\n",
       "      <td>11.21410</td>\n",
       "      <td>10.64130</td>\n",
       "      <td>11.56490</td>\n",
       "      <td>11.59360</td>\n",
       "      <td>14.57450</td>\n",
       "      <td>21.89650</td>\n",
       "      <td>18.88010</td>\n",
       "      <td>...</td>\n",
       "      <td>123.6430</td>\n",
       "      <td>125.9740</td>\n",
       "      <td>139.2400</td>\n",
       "      <td>153.2650</td>\n",
       "      <td>182.9510</td>\n",
       "      <td>184.4680</td>\n",
       "      <td>194.0290</td>\n",
       "      <td>184.9250</td>\n",
       "      <td>178.1220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.62390</td>\n",
       "      <td>13.76380</td>\n",
       "      <td>23.007000</td>\n",
       "      <td>24.28930</td>\n",
       "      <td>26.28510</td>\n",
       "      <td>17.68540</td>\n",
       "      <td>20.63140</td>\n",
       "      <td>21.38260</td>\n",
       "      <td>22.84940</td>\n",
       "      <td>30.51300</td>\n",
       "      <td>...</td>\n",
       "      <td>32.3622</td>\n",
       "      <td>32.5666</td>\n",
       "      <td>27.6383</td>\n",
       "      <td>24.4480</td>\n",
       "      <td>37.8403</td>\n",
       "      <td>28.3031</td>\n",
       "      <td>37.9221</td>\n",
       "      <td>32.1755</td>\n",
       "      <td>29.6770</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   flux_nb455  flux_nb465  flux_nb475  flux_nb485  flux_nb495  flux_nb505  \\\n",
       "0     1.92986    -3.32708    0.164769     6.26762     8.14376     2.86916   \n",
       "1    22.52590    28.05860   24.309400    25.30680    26.02040    27.41660   \n",
       "2    15.32470    11.69490   14.104500    11.29830     9.14734     8.11625   \n",
       "3     5.50019    14.03800   14.837800    11.21410    10.64130    11.56490   \n",
       "4    19.62390    13.76380   23.007000    24.28930    26.28510    17.68540   \n",
       "\n",
       "   flux_nb515  flux_nb525  flux_nb535  flux_nb545  ...    flux_nb765  \\\n",
       "0     6.63043     6.10373     6.27706     5.53524  ...       21.6058   \n",
       "1    28.58850    28.42400    26.79760    35.83690  ...       30.5420   \n",
       "2     9.37619    14.95640    16.38960    15.68470  ...       71.6377   \n",
       "3    11.59360    14.57450    21.89650    18.88010  ...      123.6430   \n",
       "4    20.63140    21.38260    22.84940    30.51300  ...       32.3622   \n",
       "\n",
       "   flux_nb775  flux_nb785  flux_nb795  flux_nb805  flux_nb815  flux_nb825  \\\n",
       "0     19.4299     26.7860     21.5572     14.8760     22.3911     25.3999   \n",
       "1     33.6569     33.5219     32.0281     39.6865     35.2799     29.7202   \n",
       "2     60.8721     75.7418     70.2047     80.8307     81.7389     85.5125   \n",
       "3    125.9740    139.2400    153.2650    182.9510    184.4680    194.0290   \n",
       "4     32.5666     27.6383     24.4480     37.8403     28.3031     37.9221   \n",
       "\n",
       "   flux_nb835  flux_nb845  class  \n",
       "0     24.3821     25.0108      0  \n",
       "1     37.5383     37.5809      0  \n",
       "2     94.8986     85.1857      0  \n",
       "3    184.9250    178.1220      0  \n",
       "4     32.1755     29.6770      0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing=pd.concat([starTraining,galaxyTraining])\n",
    "test_target=testing['class']\n",
    "testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    31654\n",
       "0     6286\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds=classifier.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8332894043226147"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = (preds > 0.5)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing['check']=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     37883\n",
       "False       57\n",
       "Name: check, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing['check'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
